{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "import os\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pylab as pylab\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from msig import Motif, NullModel\n",
    "\n",
    "params = {\"legend.fontsize\": \"xx-large\", \"axes.labelsize\": 20}\n",
    "pylab.rcParams.update(params)\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>back_x</th>\n",
       "      <th>back_y</th>\n",
       "      <th>back_z</th>\n",
       "      <th>thigh_x</th>\n",
       "      <th>thigh_y</th>\n",
       "      <th>thigh_z</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-12 00:00:00.000</th>\n",
       "      <td>-0.830025</td>\n",
       "      <td>0.012523</td>\n",
       "      <td>-0.028120</td>\n",
       "      <td>-0.952283</td>\n",
       "      <td>0.440106</td>\n",
       "      <td>-0.227452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-12 00:00:00.250</th>\n",
       "      <td>-1.001197</td>\n",
       "      <td>0.065278</td>\n",
       "      <td>-0.019865</td>\n",
       "      <td>-0.988458</td>\n",
       "      <td>-0.041124</td>\n",
       "      <td>-0.258311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-12 00:00:00.500</th>\n",
       "      <td>-1.032066</td>\n",
       "      <td>0.048774</td>\n",
       "      <td>-0.021190</td>\n",
       "      <td>-0.955791</td>\n",
       "      <td>0.082831</td>\n",
       "      <td>-0.275174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-12 00:00:00.750</th>\n",
       "      <td>-1.000332</td>\n",
       "      <td>0.056629</td>\n",
       "      <td>-0.018160</td>\n",
       "      <td>-0.955539</td>\n",
       "      <td>0.115507</td>\n",
       "      <td>-0.251909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-12 00:00:01.000</th>\n",
       "      <td>-1.016547</td>\n",
       "      <td>0.061868</td>\n",
       "      <td>0.001586</td>\n",
       "      <td>-0.966886</td>\n",
       "      <td>0.109529</td>\n",
       "      <td>-0.251787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-12 02:23:35.500</th>\n",
       "      <td>-1.005256</td>\n",
       "      <td>-0.050133</td>\n",
       "      <td>-0.095170</td>\n",
       "      <td>-0.787561</td>\n",
       "      <td>-0.543610</td>\n",
       "      <td>-0.198609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-12 02:23:35.750</th>\n",
       "      <td>-1.054037</td>\n",
       "      <td>-0.158100</td>\n",
       "      <td>-0.093967</td>\n",
       "      <td>-0.947401</td>\n",
       "      <td>0.030492</td>\n",
       "      <td>-0.225650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-12 02:23:36.000</th>\n",
       "      <td>-0.983352</td>\n",
       "      <td>-0.131485</td>\n",
       "      <td>-0.125687</td>\n",
       "      <td>-0.989039</td>\n",
       "      <td>-0.054174</td>\n",
       "      <td>-0.039868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-12 02:23:36.250</th>\n",
       "      <td>-1.007299</td>\n",
       "      <td>-0.179549</td>\n",
       "      <td>-0.194087</td>\n",
       "      <td>-1.013029</td>\n",
       "      <td>-0.127364</td>\n",
       "      <td>-0.097164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-12 02:23:36.500</th>\n",
       "      <td>-0.953996</td>\n",
       "      <td>-0.106440</td>\n",
       "      <td>-0.192335</td>\n",
       "      <td>-1.185141</td>\n",
       "      <td>0.051942</td>\n",
       "      <td>-0.149504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34467 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           back_x    back_y    back_z   thigh_x   thigh_y  \\\n",
       "timestamp                                                                   \n",
       "2019-01-12 00:00:00.000 -0.830025  0.012523 -0.028120 -0.952283  0.440106   \n",
       "2019-01-12 00:00:00.250 -1.001197  0.065278 -0.019865 -0.988458 -0.041124   \n",
       "2019-01-12 00:00:00.500 -1.032066  0.048774 -0.021190 -0.955791  0.082831   \n",
       "2019-01-12 00:00:00.750 -1.000332  0.056629 -0.018160 -0.955539  0.115507   \n",
       "2019-01-12 00:00:01.000 -1.016547  0.061868  0.001586 -0.966886  0.109529   \n",
       "...                           ...       ...       ...       ...       ...   \n",
       "2019-01-12 02:23:35.500 -1.005256 -0.050133 -0.095170 -0.787561 -0.543610   \n",
       "2019-01-12 02:23:35.750 -1.054037 -0.158100 -0.093967 -0.947401  0.030492   \n",
       "2019-01-12 02:23:36.000 -0.983352 -0.131485 -0.125687 -0.989039 -0.054174   \n",
       "2019-01-12 02:23:36.250 -1.007299 -0.179549 -0.194087 -1.013029 -0.127364   \n",
       "2019-01-12 02:23:36.500 -0.953996 -0.106440 -0.192335 -1.185141  0.051942   \n",
       "\n",
       "                          thigh_z  \n",
       "timestamp                          \n",
       "2019-01-12 00:00:00.000 -0.227452  \n",
       "2019-01-12 00:00:00.250 -0.258311  \n",
       "2019-01-12 00:00:00.500 -0.275174  \n",
       "2019-01-12 00:00:00.750 -0.251909  \n",
       "2019-01-12 00:00:01.000 -0.251787  \n",
       "...                           ...  \n",
       "2019-01-12 02:23:35.500 -0.198609  \n",
       "2019-01-12 02:23:35.750 -0.225650  \n",
       "2019-01-12 02:23:36.000 -0.039868  \n",
       "2019-01-12 02:23:36.250 -0.097164  \n",
       "2019-01-12 02:23:36.500 -0.149504  \n",
       "\n",
       "[34467 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the data\n",
    "data = pd.read_csv(\"../data/activityrecognition/S008.csv\")\n",
    "# change timestamp from float to datetime\n",
    "data[\"timestamp\"] = pd.to_datetime(data[\"timestamp\"])\n",
    "data = data.set_index(\"timestamp\")\n",
    "# resample the data to 4 per 1 second\n",
    "sr = 4\n",
    "data = data.resample(\"0.25s\").last().ffill()\n",
    "# replace labels coding\n",
    "data[\"label\"] = data[\"label\"].astype(int)\n",
    "data[\"label\"] = data[\"label\"].map(\n",
    "    {1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 13: 9, 14: 11, 130: 10, 140: 12}\n",
    ")\n",
    "labels = data[\"label\"]\n",
    "data = data.drop(columns=[\"label\"])\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = \"../results/activityrecognition/\" + str(sr)\n",
    "# create folders in results path\n",
    "if not os.path.exists(results_path):\n",
    "    os.makedirs(results_path + \"/mp\")\n",
    "    os.makedirs(results_path + \"/mp_indices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = data.columns\n",
    "# stats_table = pd.DataFrame()\n",
    "# resids = {}\n",
    "\n",
    "# # get the data for those features\n",
    "# for data_feature in features:\n",
    "#     print(data_feature)\n",
    "#     time_serie = data[data_feature]\n",
    "#     # 5hz, 1 second is 4 samples, 1 minute is 300 samples, 1 hour is 18000 samples\n",
    "#     res = MSTL(\n",
    "#         time_serie, periods=[int(sr * 60), int(sr * 60 * 60)]\n",
    "#     ).fit()  # seasonal period is 1 minute, 1 hour\n",
    "#     resids[data_feature] = res.resid\n",
    "\n",
    "#     var_resid = np.var(res.resid)\n",
    "#     var_observed = np.var(res.observed)\n",
    "#     trend_strength = max(0, 1 - (var_resid / np.var(res.trend + res.resid)))\n",
    "#     noise_strength = var_resid / var_observed\n",
    "\n",
    "#     seasonal_individial_strengths = {}\n",
    "#     for period in res.seasonal:\n",
    "#         seasonal_individial_strengths[\"F_\" + str(period)] = max(\n",
    "#             0, 1 - (var_resid / np.var(res.seasonal[period] + res.resid))\n",
    "#         )\n",
    "#     seasonal_strength = max(\n",
    "#         0, 1 - (var_resid / np.var(res.seasonal.sum(axis=1) + res.resid))\n",
    "#     )\n",
    "\n",
    "#     stats_df = {\n",
    "#         \"Feature\": data_feature,\n",
    "#         \"F_T\": round(trend_strength, 3),\n",
    "#         \"F_S\": round(seasonal_strength, 3),\n",
    "#         \"F_R\": round(noise_strength, 3),\n",
    "#     }\n",
    "\n",
    "#     # add individual seasonal strengths to stats_df, rounded with 3 decimals\n",
    "#     for period in seasonal_individial_strengths:\n",
    "#         stats_df[period] = round(seasonal_individial_strengths[period], 3)\n",
    "\n",
    "#     stats_table = pd.concat(\n",
    "#         [stats_table, pd.DataFrame(stats_df, index=[0])], ignore_index=True\n",
    "#     )\n",
    "\n",
    "# pd.DataFrame(resids).to_csv(results_path + \"/resids.csv\", index=True)\n",
    "# stats_table = stats_table.sort_values(by=[\"F_R\"], ascending=False)\n",
    "# stats_table.to_csv(results_path + \"/decomposition_summary.csv\", index=False)\n",
    "# stats_table.head().to_latex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multivar_subsequence_complexity(x):\n",
    "    # complexity for multivariate time series can be calculated as the sum of the complexity of each dimension\n",
    "    return np.sum(np.sqrt(np.sum(np.square(np.diff(x)), axis=1)))\n",
    "\n",
    "\n",
    "def table_summary_motifs(\n",
    "    motif_indices,\n",
    "    motif_distances,\n",
    "    motif_subspaces,\n",
    "    data,\n",
    "    k_distances,\n",
    "    m,\n",
    "    normalize,\n",
    "    max_allowed_dist,\n",
    "):\n",
    "    mp_stats_table = pd.DataFrame(\n",
    "        columns=[\n",
    "            \"ID\",\n",
    "            \"k_distances\",\n",
    "            \"Features\",\n",
    "            \"m\",\n",
    "            \"#Matches\",\n",
    "            \"Indices\",\n",
    "            \"max(dists)\",\n",
    "            \"min(dists)\",\n",
    "            \"med(dists)\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    motif_index = 0\n",
    "\n",
    "    n_vars, n_time = data.shape\n",
    "\n",
    "    if normalize:\n",
    "        data = (data - np.mean(data, axis=1)[:, np.newaxis]) / np.std(data, axis=1)[\n",
    "            :, np.newaxis\n",
    "        ]\n",
    "\n",
    "    dtypes = [float] * len(data)\n",
    "    model_empirical = NullModel(data, dtypes, model=\"empirical\")\n",
    "\n",
    "    for motif_indice, match_indices in enumerate(motif_indices):\n",
    "        dimensions = motif_subspaces[motif_indice]\n",
    "\n",
    "        # remove filling values of -1 and Nans from motif_indices and match_distances\n",
    "        match_indices = match_indices[match_indices != -1]\n",
    "        match_distances = motif_distances[motif_indice]\n",
    "        match_distances = match_distances[~np.isnan(match_distances)]\n",
    "\n",
    "        # if is empty, skip\n",
    "        if len(match_indices) == 0:\n",
    "            continue\n",
    "\n",
    "        excl_zone = np.ceil(m / config.STUMPY_EXCL_ZONE_DENOM)\n",
    "\n",
    "        # remove trivial matches\n",
    "        non_trivial_matches = []\n",
    "        for indice in match_indices:\n",
    "            trivial = False\n",
    "            for indice_new in non_trivial_matches:\n",
    "                if abs(indice - indice_new) <= excl_zone:\n",
    "                    trivial = True\n",
    "                    break\n",
    "            if not trivial:\n",
    "                non_trivial_matches.append(indice)\n",
    "        match_indices = non_trivial_matches\n",
    "\n",
    "        max_possible_matches = int(np.floor((n_time - m) / excl_zone + 1))\n",
    "\n",
    "        # get the multidim time serie motif in the dimensions\n",
    "        multivar_subsequence = data[dimensions][\n",
    "            :, match_indices[0] : match_indices[0] + m\n",
    "        ]\n",
    "\n",
    "        # minmax normalize subsequence\n",
    "        epsilon = 1e-10  # to avoid division by zero\n",
    "        min_values = multivar_subsequence.min(axis=1, keepdims=True)\n",
    "        max_values = multivar_subsequence.max(axis=1, keepdims=True)\n",
    "        normalized_multivar_subsequence = (multivar_subsequence - min_values) / (\n",
    "            max_values - min_values + epsilon\n",
    "        )\n",
    "        ce_norm_subsequence = multivar_subsequence_complexity(\n",
    "            normalized_multivar_subsequence\n",
    "        )\n",
    "        norm_ce_norm_subsequence = ce_norm_subsequence / (\n",
    "            np.sqrt(len(multivar_subsequence[0]) - 1) * len(dimensions)\n",
    "        )\n",
    "\n",
    "        max_dist = np.max(match_distances)\n",
    "        min_dist = np.min(match_distances[1:])\n",
    "\n",
    "        if k_distances is None:  # consider all matches\n",
    "            med_dist = np.median(match_distances[1:])\n",
    "        else:  # consider only the k closest matches\n",
    "            med_dist = np.median(match_distances[1 : k_distances + 1])\n",
    "\n",
    "        # np.nanmax([np.nanmean(D) - 2.0 * np.nanstd(D), np.nanmin(D)])\n",
    "        if max_allowed_dist is None:\n",
    "            # D The distance profile of `Q` with `T`. It is a 1D numpy array of size\n",
    "            # `len(T)-len(Q)+1`, where `D[i]` is the distance between query `Q` and\n",
    "            # `T[i : i + len(Q)]`\n",
    "            D = np.empty((n_vars, n_time - m + 1))\n",
    "            for i in range(n_vars):\n",
    "                D[i, :] = stumpy.mass(\n",
    "                    multivar_subsequence[i], data[i], normalize=normalize\n",
    "                )\n",
    "            D = np.mean(D, axis=0)\n",
    "            D_copy = D.copy().astype(np.float64)\n",
    "            D_copy[np.isinf(D_copy)] = np.nan\n",
    "            motif_max_allowed_dist = np.nanmax(\n",
    "                [np.nanmean(D_copy) - 2.0 * np.nanstd(D_copy), np.nanmin(D_copy)]\n",
    "            )\n",
    "        else:\n",
    "            motif_max_allowed_dist = max_allowed_dist\n",
    "\n",
    "        unified_weights = \"0.33,0.33,0.33\"\n",
    "        w1, w2, w3 = map(float, unified_weights.split(\",\"))\n",
    "        unified = (\n",
    "            w1 * (1 - (med_dist / motif_max_allowed_dist))\n",
    "            + w2 * (len(match_indices) / max_possible_matches)\n",
    "            + w3 * norm_ce_norm_subsequence\n",
    "        )\n",
    "\n",
    "        # remove timepoints from time series in match all indices + m\n",
    "        time_series_nomatches = data.copy()\n",
    "        # list of indexes to remove\n",
    "        indexes_to_remove = [\n",
    "            i for index in match_indices for i in range(index, index + m)\n",
    "        ]\n",
    "        # put zero in the indexes to remove\n",
    "        time_series_nomatches[:, indexes_to_remove] = 0\n",
    "\n",
    "        # calculate variance explained by the motif\n",
    "        vars_explained = []\n",
    "        for i in range(len(dimensions)):\n",
    "            vars_explained.append(\n",
    "                100\n",
    "                * (\n",
    "                    1\n",
    "                    - (\n",
    "                        np.mean(np.abs(time_series_nomatches[i]))\n",
    "                        / np.mean(np.abs(data[i]))\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "        variance_explained = np.mean(vars_explained)\n",
    "\n",
    "        # data features are now the ones in the dimensions\n",
    "        used_features = [f\"{dimension}\" for dimension in dimensions]\n",
    "\n",
    "        # max_delta = motif_max_allowed_dist # (worst case) max_dist = sqrt(max_delta^2) <=> max_delta = max_dist\n",
    "        max_delta = math.sqrt(motif_max_allowed_dist**2 / m)\n",
    "        delta_thresholds = [max_delta] * len(data)\n",
    "\n",
    "        #########SIG#########\n",
    "        motif = Motif(\n",
    "            multivar_subsequence, dimensions, delta_thresholds, len(match_indices)\n",
    "        )\n",
    "        p = motif.set_pattern_probability(model_empirical, vars_indep=True)\n",
    "        pvalue = motif.set_significance(\n",
    "            max_possible_matches, n_vars, idd_correction=False\n",
    "        )\n",
    "\n",
    "        stats_df = {\n",
    "            \"ID\": str(motif_index),\n",
    "            \"k\": len(dimensions),\n",
    "            \"Features\": \",\".join(used_features),\n",
    "            \"m\": m,\n",
    "            \"#Matches\": len(match_indices) - 1,\n",
    "            \"Indices\": match_indices,\n",
    "            \"max(dists)\": np.around(max_dist, 3),\n",
    "            \"min(dists)\": np.around(min_dist, 3),\n",
    "            \"med(dists)\": np.around(med_dist, 3),\n",
    "            \"CE\": np.around(norm_ce_norm_subsequence, 3),\n",
    "            \"Score Unified\": np.around(unified, 3),\n",
    "            \"Explained Var(%)\": np.around(variance_explained, 2),\n",
    "            \"P\": p,\n",
    "            \"p-value\": pvalue,\n",
    "        }\n",
    "\n",
    "        mp_stats_table = (\n",
    "            pd.DataFrame.from_records([stats_df])\n",
    "            if mp_stats_table.empty\n",
    "            else pd.concat(\n",
    "                [mp_stats_table, pd.DataFrame.from_records([stats_df])],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        motif_index += 1\n",
    "    return mp_stats_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# motif discovery\n",
    "import stumpy\n",
    "from stumpy import config\n",
    "\n",
    "config.STUMPY_EXCL_ZONE_DENOM = 2  # r = np.ceil(m/2)\n",
    "top_k_mp = 1\n",
    "include = None\n",
    "normalize = False\n",
    "subsequence_lengths = [sr * 15, sr * 30, sr * 60 * 1]  # 15s, 30s, 1m\n",
    "\n",
    "resids = pd.read_csv(results_path + \"/resids.csv\", index_col=0).T\n",
    "\n",
    "for m in subsequence_lengths:\n",
    "    mp, mp_indices = stumpy.mstump(resids.values, m, normalize=normalize)\n",
    "    np.save(\n",
    "        results_path\n",
    "        + \"/mp/normalize={}_topkmp={}_m={}_multivariate.npy\".format(\n",
    "            normalize, top_k_mp, m\n",
    "        ),\n",
    "        mp,\n",
    "        allow_pickle=True,\n",
    "    )\n",
    "    np.save(\n",
    "        results_path\n",
    "        + \"/mp_indices/normalize={}_topkmp={}_m={}_multivariate.npy\".format(\n",
    "            normalize, top_k_mp, m\n",
    "        ),\n",
    "        mp_indices,\n",
    "        allow_pickle=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_distances = None\n",
    "min_neighbors = 2\n",
    "cutoff = np.inf\n",
    "max_matches = 99999\n",
    "max_distance = None\n",
    "max_motifs = 99999\n",
    "k = None\n",
    "\n",
    "mp_stats_table = pd.DataFrame()\n",
    "for m in subsequence_lengths:\n",
    "    print(m)\n",
    "    X = resids.values\n",
    "    mp = np.load(\n",
    "        results_path\n",
    "        + \"/mp/normalize={}_topkmp={}_m={}_multivariate.npy\".format(\n",
    "            normalize, top_k_mp, m\n",
    "        ),\n",
    "        allow_pickle=True,\n",
    "    )\n",
    "    mp_indices = np.load(\n",
    "        results_path\n",
    "        + \"/mp_indices/normalize={}_topkmp={}_m={}_multivariate.npy\".format(\n",
    "            normalize, top_k_mp, m\n",
    "        ),\n",
    "        allow_pickle=True,\n",
    "    )\n",
    "\n",
    "    motif_distances, motif_indices, motif_subspaces, motif_mdls = stumpy.mmotifs(\n",
    "        X,\n",
    "        mp,\n",
    "        mp_indices,\n",
    "        min_neighbors=min_neighbors,\n",
    "        max_distance=max_distance,\n",
    "        cutoffs=np.inf,\n",
    "        max_matches=max_matches,\n",
    "        max_motifs=max_motifs,\n",
    "        k=k,\n",
    "        include=include,\n",
    "        normalize=normalize,\n",
    "    )\n",
    "\n",
    "    if len(motif_indices[0]) == 0:\n",
    "        continue\n",
    "    table = table_summary_motifs(\n",
    "        motif_indices, motif_distances, motif_subspaces, X, m, normalize, max_distance\n",
    "    )\n",
    "    logging.info(\n",
    "        \"m:{}, #Motifs:{}, Sig:{}\".format(\n",
    "            m, len(motif_indices), np.sum(table[\"p-value\"] < 0.001)\n",
    "        )\n",
    "    )\n",
    "    # hochberg procedure\n",
    "    p_values = table[\"p-value\"].to_numpy()\n",
    "    critical_value = NullModel.hochberg_critical_value(p_values, 0.05)\n",
    "    sig = (\n",
    "        table[\"p-value\"] < critical_value\n",
    "        if critical_value != 0\n",
    "        else table[\"p-value\"] <= critical_value\n",
    "    )\n",
    "    table[\"Sig_Hochber\"] = sig\n",
    "    logging.info(\n",
    "        \"Sig after Hochberg: {}, critical value: {}\".format(np.sum(sig), critical_value)\n",
    "    )\n",
    "    mp_stats_table = (\n",
    "        table\n",
    "        if mp_stats_table.empty\n",
    "        else pd.concat([mp_stats_table, table], ignore_index=True)\n",
    "    )\n",
    "\n",
    "    mp_stats_table.to_csv(\n",
    "        results_path\n",
    "        + \"/table_motifs_normalize={}_min_neighbors={}_max_distance={}_cutoff={}_max_matches={}_max_motifs={}.csv\".format(\n",
    "            normalize, min_neighbors, max_distance, cutoff, max_matches, max_motifs\n",
    "        ),\n",
    "        index=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read motifs table\n",
    "mp_stats_table = pd.read_csv(\n",
    "    results_path\n",
    "    + \"/table_motifs_normalize={}_min_neighbors={}_max_distance={}_cutoff={}_max_matches={}_max_motifs={}.csv\".format(\n",
    "        normalize, min_neighbors, max_distance, cutoff, max_matches, max_motifs\n",
    "    )\n",
    ")\n",
    "\n",
    "# for motif in mp_stats_table\n",
    "indexes_to_remove = set()\n",
    "for index, motif in mp_stats_table.iterrows():\n",
    "    indexes_to_remove.update(\n",
    "        [\n",
    "            i\n",
    "            for index in eval(motif[\"Indices\"])\n",
    "            for i in range(int(index), int(index) + int(motif[\"m\"]))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "datetime_indexes_to_remove = resids.T.index[sorted(indexes_to_remove)]\n",
    "\n",
    "residual_nomatches = resids.T.copy()\n",
    "for feature in residual_nomatches.columns:\n",
    "    residual_nomatches.loc[datetime_indexes_to_remove, feature] = 0\n",
    "    variance_explained = 100 * (\n",
    "        1\n",
    "        - (\n",
    "            np.mean(np.abs(residual_nomatches[feature]))\n",
    "            / np.mean(np.abs(resids.T[feature]))\n",
    "        )\n",
    "    )\n",
    "    print(f\"{feature}: {variance_explained}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new table for each motif length with statistics of the motifs (number of motifs found,\n",
    "# number of significant motifs, average number of matches +- std, average of features +- std,\n",
    "# average probability +- std, average pvalue +- std)\n",
    "\n",
    "mp_stats_table = pd.read_csv(\n",
    "    results_path\n",
    "    + \"/table_motifs_normalize={}_min_neighbors={}_max_distance={}_cutoff={}_max_matches={}_max_motifs={}.csv\".format(\n",
    "        normalize, min_neighbors, max_distance, cutoff, max_matches, max_motifs\n",
    "    )\n",
    ")\n",
    "motif_lengths = mp_stats_table[\"m\"].unique()\n",
    "motif_stats_table = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"m\",\n",
    "        \"#motifs\",\n",
    "        \"avg_n_matches\",\n",
    "        \"avg_n_features\",\n",
    "        \"avg_probability\",\n",
    "        \"avg_pvalue\",\n",
    "        \"#sig_motifs(<0.01)\",\n",
    "        \"significant\",\n",
    "        \"#sig_hochberg\",\n",
    "    ]\n",
    ")\n",
    "for m in subsequence_lengths:\n",
    "    table = mp_stats_table[mp_stats_table[\"m\"] == m]\n",
    "    if table.empty:\n",
    "        continue\n",
    "    n_motifs = table.shape[0]\n",
    "    n_sig_motifs_0001 = table[table[\"p-value\"] < 0.001].shape[0]\n",
    "    n_sig_motifs_hochberg = table[table[\"Sig_Hochber\"]].shape[0]\n",
    "    avg_n_matches = (\n",
    "        round(table[\"#Matches\"].mean(), 2),\n",
    "        round(table[\"#Matches\"].std(), 3),\n",
    "    )\n",
    "    avg_n_features = round(table[\"k\"].mean(), 2), round(table[\"k\"].std(), 3)\n",
    "    avg_probability = table[\"P\"].mean(), table[\"P\"].std()\n",
    "    avg_pvalue = table[\"p-value\"].mean(), table[\"p-value\"].std()\n",
    "\n",
    "    stats_df = {\n",
    "        \"m\": m,\n",
    "        \"#motifs\": n_motifs,\n",
    "        \"#sig_motifs(<0.001)\": n_sig_motifs_0001,\n",
    "        \"significant\": (n_sig_motifs_0001 * 100) / n_motifs,\n",
    "        \"avg_n_matches\": avg_n_matches,\n",
    "        \"avg_n_features\": avg_n_features,\n",
    "    }\n",
    "\n",
    "    motif_stats_table = (\n",
    "        pd.DataFrame.from_records([stats_df])\n",
    "        if motif_stats_table.empty\n",
    "        else pd.concat(\n",
    "            [motif_stats_table, pd.DataFrame.from_records([stats_df])],\n",
    "            ignore_index=True,\n",
    "        )\n",
    "    )\n",
    "print(motif_stats_table.to_latex(index=False, float_format=\"%.3f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_stats_table = pd.read_csv(\n",
    "    results_path\n",
    "    + \"/table_motifs_normalize={}_min_neighbors={}_max_distance={}_cutoff={}_max_matches={}_max_motifs={}.csv\".format(\n",
    "        normalize, min_neighbors, max_distance, cutoff, max_matches, max_motifs\n",
    "    )\n",
    ")\n",
    "# excluded p-value > 0.001\n",
    "mp_stats_table = mp_stats_table[mp_stats_table[\"p-value\"] < 0.001]\n",
    "subsequence_lengths = mp_stats_table[\"m\"].unique()\n",
    "for m in subsequence_lengths:\n",
    "    print(\"########## m:{} #########\".format(m))\n",
    "    top_motifs = mp_stats_table[mp_stats_table[\"m\"] == m]\n",
    "    top_motifs = top_motifs.sort_values(by=\"Score Unified\", ascending=False).head(5)\n",
    "    top_motifs = top_motifs[\n",
    "        [\n",
    "            \"ID\",\n",
    "            \"#Matches\",\n",
    "            \"CE\",\n",
    "            \"Score Unified\",\n",
    "            \"max(dists)\",\n",
    "            \"min(dists)\",\n",
    "            \"med(dists)\",\n",
    "            \"p-value\",\n",
    "            \"Explained Var(%)\",\n",
    "        ]\n",
    "    ]\n",
    "    top_motifs[\"p-value\"] = top_motifs[\"p-value\"].apply(lambda x: f\"{x:.2e}\")\n",
    "    print(top_motifs.to_latex(index=False, float_format=\"%.3f\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_motif(ts_list, features, m, motif_indexes, motif_name):\n",
    "    fig, axes = plt.subplots(\n",
    "        ncols=2, nrows=len(ts_list), figsize=(10, 2 * len(ts_list)), squeeze=False\n",
    "    )\n",
    "    for i in range(len(ts_list)):\n",
    "        # Plot light grey background time series\n",
    "        ts = ts_list[i]\n",
    "        feature = features[i]\n",
    "        axes[i, 1].plot(ts, color=\"black\", linewidth=0.5, alpha=0.5)\n",
    "\n",
    "        colors = plt.cm.tab20(np.linspace(0, 1, len(motif_indexes)))\n",
    "        axes[i, 0].set_prop_cycle(\"color\", colors)\n",
    "        axes[i, 1].set_prop_cycle(\"color\", colors)\n",
    "\n",
    "        # Store unique y-values from plotted motifs\n",
    "        plotted_values = set()\n",
    "\n",
    "        for index in motif_indexes:\n",
    "            subsequence_match = ts.iloc[index : index + m]\n",
    "            # if last plot\n",
    "            if i == len(ts_list) - 1:\n",
    "                # bullet point for the motif\n",
    "                axes[i, 0].plot(subsequence_match.values, marker=\"o\", markersize=5)\n",
    "            else:\n",
    "                # Plot original motif\n",
    "                axes[i, 0].plot(subsequence_match.values)\n",
    "            # Highlight motif in the original time series\n",
    "            axes[i, 1].plot(subsequence_match, linewidth=4)\n",
    "\n",
    "            # Collect unique values from the motif for y-ticks\n",
    "            plotted_values.update(subsequence_match.values)\n",
    "\n",
    "        plt.setp(axes[i, 0].xaxis.get_majorticklabels(), rotation=90)\n",
    "        # Remove x labels and ticks except for the last plot\n",
    "        if i != len(ts_list) - 1:\n",
    "            axes[i, 0].axes.get_xaxis().set_visible(False)\n",
    "            axes[i, 1].axes.get_xaxis().set_visible(False)\n",
    "\n",
    "        # Format the x-axis to show the time and rotate for better reading\n",
    "        axes[i, 1].xaxis.set_major_formatter(mdates.DateFormatter(\"%H:%M:%S\"))\n",
    "        plt.setp(axes[i, 1].xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "        axes[i, 0].set_ylabel(features[i], rotation=90, size=\"large\")\n",
    "\n",
    "    # Custom y-tick labels using labels_dict\n",
    "    labels_dict = {\n",
    "        1: \"walking\",\n",
    "        2: \"running\",\n",
    "        3: \"shuffling\",\n",
    "        4: \"stairs (ascending)\",\n",
    "        5: \"stairs (descending)\",\n",
    "        6: \"standing\",\n",
    "        7: \"sitting\",\n",
    "        8: \"lying\",\n",
    "        9: \"cycling (sit)\",\n",
    "        11: \"cycling (stand)\",\n",
    "        10: \"cycling (sit, inactive)\",\n",
    "        12: \"cycling (stand, inactive)\",\n",
    "    }\n",
    "\n",
    "    # Set custom y-tick labels for the last subplot on the left\n",
    "    if len(ts_list) > 0:\n",
    "        ax = axes[-1, 0]\n",
    "        plotted_values = sorted(plotted_values)  # Sort for orderly tick placement\n",
    "        yticklabels = [\n",
    "            labels_dict.get(int(round(val)), \"\")\n",
    "            for val in plotted_values\n",
    "            if int(round(val)) in labels_dict\n",
    "        ]\n",
    "\n",
    "        ax.set_yticks(plotted_values)\n",
    "        ax.set_yticklabels(yticklabels)\n",
    "\n",
    "        # remove y-ticks from the right plot\n",
    "        axes[-1, 1].axes.get_yaxis().set_visible(False)\n",
    "\n",
    "    # Set titles\n",
    "    axes[0, 0].set_title(\"Raw Subsequences\")\n",
    "    axes[0, 1].set_title(\"Motif in TS\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        results_path + \"/m=\" + str(m) + \"_motif_\" + str(motif_name) + \".pdf\",\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load motif statistics table\n",
    "file_path = os.path.join(\n",
    "    results_path,\n",
    "    f\"table_motifs_normalize={normalize}_min_neighbors={min_neighbors}_max_distance={max_distance}_cutoff={cutoff}_max_matches={max_matches}_max_motifs={max_motifs}.csv\",\n",
    ")\n",
    "mp_stats_table = pd.read_csv(file_path)\n",
    "\n",
    "# Extract unique subsequence lengths\n",
    "subsequence_lengths = mp_stats_table[\"m\"].unique()\n",
    "\n",
    "# Time series data\n",
    "ts = resids\n",
    "\n",
    "# Loop over each subsequence length\n",
    "for m in subsequence_lengths:\n",
    "    logging.info(f\"Motif length: {m}\")\n",
    "\n",
    "    # Filter motifs by current subsequence length\n",
    "    top_motifs = mp_stats_table[mp_stats_table[\"m\"] == m]\n",
    "\n",
    "    # Loop over each top motif\n",
    "    for top_motif in top_motifs.to_dict(orient=\"records\"):\n",
    "        # Parse dimensions and indices\n",
    "        dimensions = sorted(map(int, top_motif[\"Features\"].split(\",\")))\n",
    "        indices = sorted(map(int, top_motif[\"Indices\"].strip(\"[]\").split(\",\")))\n",
    "\n",
    "        # Extract feature names from the data columns\n",
    "        features = [data.columns[dimension] for dimension in dimensions]\n",
    "\n",
    "        # Add \"Activity\" feature and corresponding time series\n",
    "        ts_list = [resids.T[feature] for feature in features]\n",
    "        ts_list.append(labels)\n",
    "        features.append(\"Activity\")\n",
    "\n",
    "        # Generate motif name\n",
    "        motif_name = top_motif[\"ID\"]\n",
    "\n",
    "        # Check if the indices are far enough apart (delta >= 1000)\n",
    "        if any(\n",
    "            abs(indices[i] - indices[i + 1]) >= 1000 for i in range(len(indices) - 1)\n",
    "        ):\n",
    "            plot_motif(ts_list, features, m, indices, motif_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "motifsinresidualenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
