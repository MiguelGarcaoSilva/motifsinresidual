{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from msig import Motif, NullModel\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import STL, MSTL\n",
    "import matplotlib.pylab as pylab\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "params = {'legend.fontsize': 'xx-large',\n",
    "         'axes.labelsize': 20}\n",
    "pylab.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>back_x</th>\n",
       "      <th>back_y</th>\n",
       "      <th>back_z</th>\n",
       "      <th>thigh_x</th>\n",
       "      <th>thigh_y</th>\n",
       "      <th>thigh_z</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-12 00:00:00</th>\n",
       "      <td>-1.000332</td>\n",
       "      <td>0.056629</td>\n",
       "      <td>-0.018160</td>\n",
       "      <td>-0.955539</td>\n",
       "      <td>0.115507</td>\n",
       "      <td>-0.251909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-12 00:00:01</th>\n",
       "      <td>-1.001483</td>\n",
       "      <td>0.079709</td>\n",
       "      <td>0.001390</td>\n",
       "      <td>-0.929451</td>\n",
       "      <td>0.120106</td>\n",
       "      <td>-0.339157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-12 00:00:02</th>\n",
       "      <td>-0.997195</td>\n",
       "      <td>0.061839</td>\n",
       "      <td>-0.030257</td>\n",
       "      <td>-0.926921</td>\n",
       "      <td>0.108350</td>\n",
       "      <td>-0.353736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-12 00:00:03</th>\n",
       "      <td>-1.003453</td>\n",
       "      <td>0.067053</td>\n",
       "      <td>-0.018227</td>\n",
       "      <td>-0.915777</td>\n",
       "      <td>0.120929</td>\n",
       "      <td>-0.391562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-12 00:00:04</th>\n",
       "      <td>-1.014147</td>\n",
       "      <td>0.069401</td>\n",
       "      <td>-0.030973</td>\n",
       "      <td>-0.918397</td>\n",
       "      <td>0.133463</td>\n",
       "      <td>-0.379393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-12 02:23:32</th>\n",
       "      <td>-0.998232</td>\n",
       "      <td>0.098795</td>\n",
       "      <td>0.127499</td>\n",
       "      <td>-0.198296</td>\n",
       "      <td>0.292665</td>\n",
       "      <td>0.908259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-12 02:23:33</th>\n",
       "      <td>-0.905140</td>\n",
       "      <td>0.043037</td>\n",
       "      <td>0.037131</td>\n",
       "      <td>-0.958983</td>\n",
       "      <td>-0.064922</td>\n",
       "      <td>-0.049335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-12 02:23:34</th>\n",
       "      <td>-1.097114</td>\n",
       "      <td>0.094986</td>\n",
       "      <td>0.066671</td>\n",
       "      <td>-0.868306</td>\n",
       "      <td>0.034518</td>\n",
       "      <td>-0.389120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-12 02:23:35</th>\n",
       "      <td>-1.054037</td>\n",
       "      <td>-0.158100</td>\n",
       "      <td>-0.093967</td>\n",
       "      <td>-0.947401</td>\n",
       "      <td>0.030492</td>\n",
       "      <td>-0.225650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-12 02:23:36</th>\n",
       "      <td>-0.953996</td>\n",
       "      <td>-0.106440</td>\n",
       "      <td>-0.192335</td>\n",
       "      <td>-1.185141</td>\n",
       "      <td>0.051942</td>\n",
       "      <td>-0.149504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8617 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       back_x    back_y    back_z   thigh_x   thigh_y  \\\n",
       "timestamp                                                               \n",
       "2019-01-12 00:00:00 -1.000332  0.056629 -0.018160 -0.955539  0.115507   \n",
       "2019-01-12 00:00:01 -1.001483  0.079709  0.001390 -0.929451  0.120106   \n",
       "2019-01-12 00:00:02 -0.997195  0.061839 -0.030257 -0.926921  0.108350   \n",
       "2019-01-12 00:00:03 -1.003453  0.067053 -0.018227 -0.915777  0.120929   \n",
       "2019-01-12 00:00:04 -1.014147  0.069401 -0.030973 -0.918397  0.133463   \n",
       "...                       ...       ...       ...       ...       ...   \n",
       "2019-01-12 02:23:32 -0.998232  0.098795  0.127499 -0.198296  0.292665   \n",
       "2019-01-12 02:23:33 -0.905140  0.043037  0.037131 -0.958983 -0.064922   \n",
       "2019-01-12 02:23:34 -1.097114  0.094986  0.066671 -0.868306  0.034518   \n",
       "2019-01-12 02:23:35 -1.054037 -0.158100 -0.093967 -0.947401  0.030492   \n",
       "2019-01-12 02:23:36 -0.953996 -0.106440 -0.192335 -1.185141  0.051942   \n",
       "\n",
       "                      thigh_z  \n",
       "timestamp                      \n",
       "2019-01-12 00:00:00 -0.251909  \n",
       "2019-01-12 00:00:01 -0.339157  \n",
       "2019-01-12 00:00:02 -0.353736  \n",
       "2019-01-12 00:00:03 -0.391562  \n",
       "2019-01-12 00:00:04 -0.379393  \n",
       "...                       ...  \n",
       "2019-01-12 02:23:32  0.908259  \n",
       "2019-01-12 02:23:33 -0.049335  \n",
       "2019-01-12 02:23:34 -0.389120  \n",
       "2019-01-12 02:23:35 -0.225650  \n",
       "2019-01-12 02:23:36 -0.149504  \n",
       "\n",
       "[8617 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read the data\n",
    "data = pd.read_csv(\"../data/activityrecognition/S008.csv\")\n",
    "#change timestamp from float to datetime\n",
    "data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "data = data.set_index('timestamp')\n",
    "data = data.drop(columns=['label'])\n",
    "#resample the data to 1 second\n",
    "sr = 1\n",
    "data = data.resample('1s').last().ffill()\n",
    "#get the first second records\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>F_T</th>\n",
       "      <th>F_S</th>\n",
       "      <th>F_R</th>\n",
       "      <th>F_seasonal_60</th>\n",
       "      <th>F_seasonal_3600</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>back_x</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.866</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.285</td>\n",
       "      <td>0.854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>back_y</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.804</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>back_z</td>\n",
       "      <td>0.507</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thigh_x</td>\n",
       "      <td>0.522</td>\n",
       "      <td>0.903</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>thigh_y</td>\n",
       "      <td>0.339</td>\n",
       "      <td>0.813</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.295</td>\n",
       "      <td>0.789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>thigh_z</td>\n",
       "      <td>0.529</td>\n",
       "      <td>0.857</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.851</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature    F_T    F_S    F_R  F_seasonal_60  F_seasonal_3600\n",
       "0   back_x  0.219  0.866  0.131          0.285            0.854\n",
       "1   back_y  0.438  0.804  0.184          0.312            0.776\n",
       "2   back_z  0.507  0.822  0.163          0.115            0.816\n",
       "3  thigh_x  0.522  0.903  0.092          0.185            0.900\n",
       "4  thigh_y  0.339  0.813  0.170          0.295            0.789\n",
       "5  thigh_z  0.529  0.857  0.126          0.166            0.851"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#resid\n",
    "stats_table = pd.DataFrame()\n",
    "resids = {}\n",
    "\n",
    "#get the data for those features\n",
    "for data_feature in data.columns:\n",
    "    time_serie = data[data_feature].asfreq('s')\n",
    "    #1hz, 1 second is 1 samples, 1 minute is 60 samples, 1 hour is 3600 samples\n",
    "    res = MSTL(time_serie, periods=[60,3600]).fit() #seasonal period is 1 minute, 1 hour\n",
    "    resids[data_feature] = res.resid\n",
    "\n",
    "    var_resid = np.var(res.resid)\n",
    "    var_observed = np.var(res.observed)\n",
    "    trend_strength = max(0, 1 - (var_resid/np.var(res.trend+res.resid)))\n",
    "    noise_strength = var_resid/var_observed\n",
    "\n",
    "    seasonal_individial_strengths = {}\n",
    "    for period in res.seasonal:\n",
    "        seasonal_individial_strengths[\"F_\"+str(period)] = max(0, 1 - (var_resid/np.var(res.seasonal[period] + res.resid)))\n",
    "    seasonal_strength = max(0, 1 - (var_resid/np.var(res.seasonal.sum(axis=1) + res.resid)))\n",
    "\n",
    "    stats_df = {\"Feature\": data_feature,\n",
    "                \"F_T\": round(trend_strength, 3),\n",
    "                \"F_S\": round(seasonal_strength, 3), \"F_R\": round(noise_strength, 3)}\n",
    "        \n",
    "    #add individual seasonal strengths to stats_df, rounded with 3 decimals\n",
    "    for period in seasonal_individial_strengths:\n",
    "        stats_df[period] = round(seasonal_individial_strengths[period], 3)\n",
    "            \n",
    "    stats_table = pd.concat([stats_table, pd.DataFrame(stats_df, index=[0])], ignore_index=True)\n",
    "\n",
    "pd.DataFrame(resids).to_csv(\"../results/activityrecognition/resids/resids.csv\", index=False)\n",
    "stats_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#motif discovery\n",
    "import stumpy\n",
    "from stumpy import config\n",
    "\n",
    "config.STUMPY_EXCL_ZONE_DENOM = 2 # r = np.ceil(m/2)\n",
    "include = None\n",
    "normalize = False\n",
    "subsequence_lengths = [sr*30, sr*60, sr*60*10] # 30s, 1m, 10m "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "resids = pd.read_csv(\"../results/activityrecognition/resids/resids.csv\").T\n",
    "\n",
    "for m in subsequence_lengths:\n",
    "    out = stumpy.mstump(resids.values, m, normalize=normalize)\n",
    "    np.save('../results/activityrecognition/mp/normalized={}_m={}_multivariate.npy'.format(normalize,m), out, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multivar_subsequence_complexity(x):\n",
    "    # complexity for multivariate time series can be calculated as the sum of the complexity of each dimension\n",
    "    return np.sum(np.sqrt(np.sum(np.square(np.diff(x)), axis=1)))\n",
    "\n",
    "def table_summary_motifs(motif_indices, motif_distances, motif_subspaces, data, m, normalize, max_allowed_dist):\n",
    "    mp_stats_table = pd.DataFrame(columns=[\"ID\", \"k\", \"Features\", \"m\", \"#Matches\", \"Indices\", \"max(dists)\", \"min(dists)\", \"med(dists)\"])\n",
    "\n",
    "    motif_index = 0\n",
    "\n",
    "    n_vars, n_time = data.shape\n",
    "\n",
    "    if normalize:\n",
    "        data = (data - np.mean(data, axis=1)[:, np.newaxis]) / np.std(data, axis=1)[:, np.newaxis]\n",
    "    \n",
    "    dtypes = [float] * len(data)\n",
    "    model_empirical = NullModel(data, dtypes, model=\"empirical\")\n",
    "\n",
    "    for motif_indice, match_indices in enumerate(motif_indices):\n",
    "\n",
    "        dimensions = motif_subspaces[motif_indice]\n",
    "            \n",
    "        #remove filling values of -1 and Nans from motif_indices and match_distances\n",
    "        match_indices = match_indices[match_indices != -1]\n",
    "        match_distances = motif_distances[motif_indice]\n",
    "        match_distances = match_distances[~np.isnan(match_distances)]\n",
    "\n",
    "        #if is empty, skip\n",
    "        if len(match_indices) == 0:\n",
    "            continue\n",
    "        \n",
    "        #remove trivial matches  \n",
    "        non_trivial_matches = []\n",
    "        for indice in match_indices:\n",
    "           trivial = False\n",
    "           for indice_new in non_trivial_matches:\n",
    "               if abs(indice - indice_new) <= m/2:\n",
    "                   trivial = True\n",
    "                   break\n",
    "           if not trivial:\n",
    "               non_trivial_matches.append(indice)\n",
    "        match_indices = non_trivial_matches\n",
    "\n",
    "        max_possible_matches = int(np.floor((n_time-m)/np.ceil(m/2)+1))\n",
    "\n",
    "        #get the multidim time serie motif in the dimensions\n",
    "        multivar_subsequence = data[dimensions][:,match_indices[0]:match_indices[0]+m]\n",
    "\n",
    "        #minmax normalize subsequence\n",
    "        epsilon = 1e-10 # to avoid division by zero\n",
    "        min_values = multivar_subsequence.min(axis=1, keepdims=True)\n",
    "        max_values = multivar_subsequence.max(axis=1, keepdims=True)\n",
    "        normalized_multivar_subsequence = (multivar_subsequence - min_values) / (max_values - min_values + epsilon)\n",
    "        ce_norm_subsequence = multivar_subsequence_complexity(normalized_multivar_subsequence)\n",
    "        norm_ce_norm_subsequence = ce_norm_subsequence/(np.sqrt(len(multivar_subsequence[0])-1)*len(dimensions))\n",
    "\n",
    "        max_dist = np.max(match_distances)\n",
    "        min_dist = np.min(match_distances[1:])\n",
    "        avg_dist = np.mean(match_distances[1:])\n",
    "        std_dist = np.std(match_distances[1:])\n",
    "        med_dist = np.median(match_distances[1:])\n",
    "        \n",
    "        #D is distance profile between the motif pattern and Time serie\n",
    "        if max_allowed_dist is None:\n",
    "            D = np.empty((n_time-m+1, len(dimensions)))\n",
    "            for i, dimension in enumerate(dimensions):\n",
    "                D[:,i] = stumpy.mass(multivar_subsequence[i], data[dimension], normalize=normalize)\n",
    "            D = np.mean(D, axis=1)\n",
    "            max_allowed_dist = max(np.nanmax([np.nanmean(D) - 2.0 * np.nanstd(D), np.nanmin(D)]), epsilon)\n",
    "        \n",
    "        unified_weights = \"0.33,0.33,0.33\"\n",
    "        excl_zone = np.ceil(m/4)\n",
    "        w1, w2, w3 = map(float, unified_weights.split(\",\"))\n",
    "        unified = w1*(1-(med_dist/max_allowed_dist)) + w2*(len(match_indices)/max_possible_matches) + w3*norm_ce_norm_subsequence\n",
    "\n",
    "        #remove timepoints from time series in match all indices + m\n",
    "        time_series_nomatches = data[dimension].copy()\n",
    "        #list of indexes to remove\n",
    "        indexes_to_remove = [i for index in match_indices for i in range(index, index + m)]\n",
    "        #put zero in the indexes to remove\n",
    "        time_series_nomatches[indexes_to_remove] = 0\n",
    "\n",
    "        #calculate variance explained by the motif\n",
    "        variance_explained = 100 * (1 - (np.var(time_series_nomatches) / np.var(data[dimension])))\n",
    "\n",
    "        #data features are now the ones in the dimensions\n",
    "        used_features = [f\"{dimension}\" for dimension in dimensions]\n",
    "\n",
    "        #max_delta = max_allowed_dist # (worst case) max_dist = sqrt(max_delta^2) <=> max_delta = max_dist\n",
    "        max_delta = math.sqrt(max_allowed_dist**2/m) \n",
    "        delta_thresholds = [max_delta]*len(data)\n",
    "        \n",
    "        #########SIG#########\n",
    "        motif = Motif(multivar_subsequence, dimensions, delta_thresholds, len(match_indices))\n",
    "        p = motif.set_pattern_probability(model_empirical, vars_indep=True)\n",
    "        pvalue = motif.set_significance(max_possible_matches, n_vars, idd_correction=False) \n",
    "\n",
    "        stats_df = {\"ID\": str(motif_index), \"k\":len(dimensions),\n",
    "                    \"Features\":\",\".join(used_features),\n",
    "                        \"m\":m,\n",
    "                    \"#Matches\": len(match_indices)-1,\n",
    "                        \"Indices\":match_indices,\n",
    "                        \"max(dists)\": np.around(max_dist,3), \"min(dists)\": np.around(min_dist,3),\n",
    "                        \"med(dists)\": np.around(med_dist,3),  \"CE\": np.around(norm_ce_norm_subsequence,3), \"Score Unified\": np.around(unified,3),\n",
    "                        \"Explained Var(%)\": np.around(variance_explained,2),\n",
    "                        \"P\": p, \"p-value\": pvalue}\n",
    "    \n",
    "        mp_stats_table = pd.DataFrame.from_records([stats_df]) if mp_stats_table.empty else pd.concat(\n",
    "            [mp_stats_table, pd.DataFrame.from_records([stats_df])], ignore_index=True)\n",
    "        \n",
    "        motif_index += 1\n",
    "    return mp_stats_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m:30, #Motifs:14\n",
      "Sig  14\n",
      "Sig after Hochberg: 13, critical value: 4.06385826468665e-77\n",
      "m:60, #Motifs:5\n",
      "Sig  5\n",
      "Sig after Hochberg: 4, critical value: 1.6116110933910169e-130\n"
     ]
    }
   ],
   "source": [
    "min_neighbors = 2\n",
    "max_distance = None\n",
    "max_matches = 99999\n",
    "max_motifs = 99999\n",
    "cutoff = np.inf\n",
    "k = None\n",
    "mp_stats_table = pd.DataFrame()\n",
    "for m in subsequence_lengths:\n",
    "    X = resids.values\n",
    "    out = np.load('../results/activityrecognition/mp/normalized=True_m={}_multivariate.npy'.format(m), allow_pickle=True)\n",
    "\n",
    "    mp, mp_indices = out[0], out[1].astype(int)\n",
    "\n",
    "    motif_distances, motif_indices, motif_subspaces, motif_mdls = stumpy.mmotifs(X, mp, mp_indices, min_neighbors=min_neighbors, max_distance=max_distance,\n",
    "                                                                    cutoffs=np.inf, max_matches=max_matches, max_motifs=max_motifs, k=k, include=include, normalize=normalize)  \n",
    "    \n",
    "    if len(motif_indices[0]) == 0:\n",
    "        continue\n",
    "    print(\"m:{}, #Motifs:{}\".format(m, len(motif_indices)))\n",
    "    table = table_summary_motifs(motif_indices, motif_distances, motif_subspaces, X, m, normalize, max_distance)\n",
    "    print(\"Sig \", np.sum(table[\"p-value\"] < 0.01))\n",
    "    #hochberg procedure\n",
    "    p_values = table[\"p-value\"].to_numpy()\n",
    "    critical_value =  NullModel.hochberg_critical_value(p_values, 0.05)\n",
    "    sig = table[\"p-value\"] < critical_value if critical_value != 0 else table[\"p-value\"] <= critical_value\n",
    "    table[\"Sig_Hochber\"] = sig\n",
    "\n",
    "    print(\"Sig after Hochberg: {}, critical value: {}\".format(np.sum(sig), critical_value))\n",
    "    mp_stats_table = table if mp_stats_table.empty else pd.concat([mp_stats_table, table], ignore_index=True)\n",
    "\n",
    "    mp_stats_table.to_csv('../results/activityrecognition/table_motifs_min_neighbors={}_max_distance={}_cutoff={}_max_matches={}_max_motifs={}.csv'.format(min_neighbors, max_distance, cutoff, max_matches, max_motifs), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m     avg_probability \u001b[38;5;241m=\u001b[39m table[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmean(), table[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mstd()    \n\u001b[1;32m     16\u001b[0m     avg_pvalue \u001b[38;5;241m=\u001b[39m table[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp-value\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmean(), table[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp-value\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mstd()\n\u001b[0;32m---> 18\u001b[0m     stats_df \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m\"\u001b[39m: m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#motifs\u001b[39m\u001b[38;5;124m\"\u001b[39m: n_motifs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#sig_motifs(<0.01)\u001b[39m\u001b[38;5;124m\"\u001b[39m: n_sig_motifs_001, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msignificant\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[43m(\u001b[49m\u001b[43mn_sig_motifs_001\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mn_motifs\u001b[49m,\n\u001b[1;32m     19\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavg_n_matches\u001b[39m\u001b[38;5;124m\"\u001b[39m: avg_n_matches}\n\u001b[1;32m     21\u001b[0m     motif_stats_table \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records([stats_df]) \u001b[38;5;28;01mif\u001b[39;00m motif_stats_table\u001b[38;5;241m.\u001b[39mempty \u001b[38;5;28;01melse\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mconcat([motif_stats_table, pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records([stats_df])], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(motif_stats_table\u001b[38;5;241m.\u001b[39mto_latex(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "#create a new table for each motif length with statistics of the motifs (number of motifs found,\n",
    "# number of significant motifs, average number of matches +- std, average of features +- std, \n",
    "#average probability +- std, average pvalue +- std)\n",
    "\n",
    "mp_stats_table = pd.read_csv('../results/activityrecognition/table_motifs_min_neighbors={}_max_distance={}_cutoff={}_max_matches={}_max_motifs={}.csv'.format(min_neighbors, max_distance, cutoff, max_matches, max_motifs))\n",
    "motif_lengths = mp_stats_table[\"m\"].unique()\n",
    "motif_stats_table = pd.DataFrame(columns=[\"m\", \"#motifs\" , \"avg_n_matches\", \"avg_n_features\",  \"avg_probability\",  \"avg_pvalue\", \"#sig_motifs(<0.01)\", \"significant\", \"#sig_hochberg\"])\n",
    "for m in subsequence_lengths:\n",
    "    table = mp_stats_table[mp_stats_table[\"m\"] == m]\n",
    "    n_motifs = table.shape[0]\n",
    "    n_sig_motifs_001 = table[table[\"p-value\"] < 0.01].shape[0]\n",
    "    n_sig_motifs_hochberg = table[table[\"Sig_Hochber\"]].shape[0]\n",
    "    avg_n_matches = round(table[\"#Matches\"].mean(),2), round(table[\"#Matches\"].std(),3)\n",
    "    avg_n_features = round(table[\"k\"].mean(),2), round(table[\"k\"].std(),3)\n",
    "    avg_probability = table[\"P\"].mean(), table[\"P\"].std()    \n",
    "    avg_pvalue = table[\"p-value\"].mean(), table[\"p-value\"].std()\n",
    "\n",
    "    stats_df = {\"m\": m, \"#motifs\": n_motifs, \"#sig_motifs(<0.01)\": n_sig_motifs_001, \"significant\":(n_sig_motifs_001*100)/n_motifs,\n",
    "                \"avg_n_matches\": avg_n_matches}\n",
    "    \n",
    "    motif_stats_table = pd.DataFrame.from_records([stats_df]) if motif_stats_table.empty else pd.concat([motif_stats_table, pd.DataFrame.from_records([stats_df])], ignore_index=True)\n",
    "print(motif_stats_table.to_latex(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_motif(ts_list, features,  m, motif_indexes, motif_name):\n",
    "\n",
    "    fig, axes = plt.subplots(ncols=2, nrows=len(ts_list), figsize=(10, 2*len(ts_list)), squeeze=False)\n",
    "    for i in range(0,len(ts_list)):\n",
    "        ts = ts_list[i]\n",
    "        #plot light grey\n",
    "        axes[i,1].plot(ts, color='black', linewidth=0.5, alpha=0.5)\n",
    "\n",
    "        colors = plt.cm.tab20(np.linspace(0, 1, len(motif_indexes)))\n",
    "        axes[i,0].set_prop_cycle('color', colors)\n",
    "        axes[i,1].set_prop_cycle('color', colors)\n",
    "\n",
    "        for index in motif_indexes:\n",
    "            subsequence_match = ts.iloc[index:index+m]\n",
    "            #original motif in the next plot with the same color\n",
    "            axes[i,0].plot(subsequence_match.values) \n",
    "            # highlight the motif in the original time serie\n",
    "            axes[i,1].plot(subsequence_match, linewidth=2)\n",
    "        \n",
    "        plt.setp(axes[i,0].xaxis.get_majorticklabels(), rotation=90)\n",
    "        #remove x labels and ticks except from last plot\n",
    "        if i != len(ts_list)-1:\n",
    "            axes[i,0].axes.get_xaxis().set_visible(False)\n",
    "            axes[i,1].axes.get_xaxis().set_visible(False)\n",
    "\n",
    "        #label x with i+index\n",
    "        plt.setp(axes[i,0].xaxis.get_majorticklabels(), rotation=90)\n",
    "\n",
    "        #format the x axis to show the time and rotate for better reading\n",
    "        axes[i,1].xaxis.set_major_formatter(mdates.DateFormatter('%H:%M:%S'))\n",
    "        plt.setp(axes[i,1].xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "        #if y label is too long, put \\n\n",
    "        if features[i] == \"Water Inlet Temperature(°C)\":\n",
    "            features[i] = \"Water Inlet\\nTemperature (°C)\"\n",
    "        if features[i] == \"Water Outlet Temperature(°C)\":\n",
    "            features[i] = \"Water Outlet\\nTemperature (°C)\"\n",
    "        axes[i,0].set_ylabel(features[i], rotation=90, size='large')\n",
    "\n",
    "\n",
    "    #title of the fig\n",
    "    axes[0,0].set_title(\"Raw Subsequences\")\n",
    "    axes[0,1].set_title(\"Motif in TS\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results/washingmachine/m='+str(m)+'_motif_'+str(motif_name)+\".pdf\",bbox_inches='tight')\n",
    "  \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot top motif\n",
    "mp_stats_table = pd.read_csv('../results/activityrecognition/table_motifs_min_neighbors={}_max_distance={}_cutoff={}_max_matches={}_max_motifs={}.csv'.format(min_neighbors, max_distance, cutoff, max_matches, max_motifs))\n",
    "subsequence_lengths = mp_stats_table[\"m\"].unique()\n",
    "ts = resids\n",
    "\n",
    "for m in subsequence_lengths:\n",
    "    print(\"Motif length: \", m)\n",
    "    top_motifs = mp_stats_table[mp_stats_table[\"m\"] == m]\n",
    "    top_motifs = top_motifs[top_motifs[\"k\"] > 1].sort_values(by=\"CE\").head(3)\n",
    "    for top_motif in top_motifs.to_dict(orient=\"records\"): \n",
    "        m = top_motif[\"m\"]\n",
    "        dimensions = top_motif[\"Features\"].split(\",\")\n",
    "        dimensions = sorted([int(dimension) for dimension in dimensions])\n",
    "        features = [data.columns[dimension] for dimension in dimensions]\n",
    "        indices = top_motif['Indices'].replace(\"[\",\"\").replace(\"]\",\"\").split(\",\")\n",
    "        indices = [int(i) for i in indices]\n",
    "        motif_name = top_motif[\"ID\"]\n",
    "        ts_list = [data[feature] for feature in features]\n",
    "        plot_motif(ts_list, features, m, indices, motif_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "motifsinresidualenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
