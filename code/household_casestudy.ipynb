{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pylab as pylab\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.seasonal import MSTL\n",
    "\n",
    "from msig import Motif, NullModel\n",
    "\n",
    "params = {\"legend.fontsize\": \"xx-large\", \"axes.labelsize\": 20}\n",
    "pylab.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miguel/miniconda3/envs/motifsinresidualenv/lib/python3.9/site-packages/ucimlrepo/fetch.py:97: DtypeWarning: Columns (2,3,4,5,6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_url)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Global_active_power</th>\n",
       "      <th>Global_reactive_power</th>\n",
       "      <th>Voltage</th>\n",
       "      <th>Global_intensity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2008-09-01 00:00:00</th>\n",
       "      <td>1.300</td>\n",
       "      <td>0.000</td>\n",
       "      <td>243.04</td>\n",
       "      <td>5.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-09-01 00:01:00</th>\n",
       "      <td>1.282</td>\n",
       "      <td>0.000</td>\n",
       "      <td>243.30</td>\n",
       "      <td>5.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-09-01 00:02:00</th>\n",
       "      <td>1.302</td>\n",
       "      <td>0.000</td>\n",
       "      <td>243.40</td>\n",
       "      <td>5.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-09-01 00:03:00</th>\n",
       "      <td>1.284</td>\n",
       "      <td>0.000</td>\n",
       "      <td>243.47</td>\n",
       "      <td>5.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-09-01 00:04:00</th>\n",
       "      <td>1.304</td>\n",
       "      <td>0.000</td>\n",
       "      <td>243.55</td>\n",
       "      <td>5.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-10-24 23:55:00</th>\n",
       "      <td>0.682</td>\n",
       "      <td>0.000</td>\n",
       "      <td>243.84</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-10-24 23:56:00</th>\n",
       "      <td>0.630</td>\n",
       "      <td>0.064</td>\n",
       "      <td>244.69</td>\n",
       "      <td>2.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-10-24 23:57:00</th>\n",
       "      <td>0.620</td>\n",
       "      <td>0.080</td>\n",
       "      <td>245.20</td>\n",
       "      <td>2.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-10-24 23:58:00</th>\n",
       "      <td>0.620</td>\n",
       "      <td>0.080</td>\n",
       "      <td>245.15</td>\n",
       "      <td>2.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-10-24 23:59:00</th>\n",
       "      <td>0.618</td>\n",
       "      <td>0.080</td>\n",
       "      <td>244.90</td>\n",
       "      <td>2.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>77760 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Global_active_power  Global_reactive_power  Voltage  \\\n",
       "timestamp                                                                  \n",
       "2008-09-01 00:00:00                1.300                  0.000   243.04   \n",
       "2008-09-01 00:01:00                1.282                  0.000   243.30   \n",
       "2008-09-01 00:02:00                1.302                  0.000   243.40   \n",
       "2008-09-01 00:03:00                1.284                  0.000   243.47   \n",
       "2008-09-01 00:04:00                1.304                  0.000   243.55   \n",
       "...                                  ...                    ...      ...   \n",
       "2008-10-24 23:55:00                0.682                  0.000   243.84   \n",
       "2008-10-24 23:56:00                0.630                  0.064   244.69   \n",
       "2008-10-24 23:57:00                0.620                  0.080   245.20   \n",
       "2008-10-24 23:58:00                0.620                  0.080   245.15   \n",
       "2008-10-24 23:59:00                0.618                  0.080   244.90   \n",
       "\n",
       "                     Global_intensity  \n",
       "timestamp                              \n",
       "2008-09-01 00:00:00               5.4  \n",
       "2008-09-01 00:01:00               5.2  \n",
       "2008-09-01 00:02:00               5.2  \n",
       "2008-09-01 00:03:00               5.2  \n",
       "2008-09-01 00:04:00               5.4  \n",
       "...                               ...  \n",
       "2008-10-24 23:55:00               2.8  \n",
       "2008-10-24 23:56:00               2.6  \n",
       "2008-10-24 23:57:00               2.6  \n",
       "2008-10-24 23:58:00               2.6  \n",
       "2008-10-24 23:59:00               2.6  \n",
       "\n",
       "[77760 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# fetch dataset\n",
    "individual_household_electric_power_consumption = fetch_ucirepo(id=235)\n",
    "\n",
    "data = individual_household_electric_power_consumption.data.features\n",
    "# date and time col to timestamp\n",
    "data[\"timestamp\"] = pd.to_datetime(\n",
    "    data[\"Date\"] + \" \" + data[\"Time\"], format=\"%d/%m/%Y %H:%M:%S\"\n",
    ")\n",
    "data = data.set_index(\"timestamp\")\n",
    "data = data.resample(\"1min\").last().ffill()\n",
    "# largest sequence without nans\n",
    "data = data.loc[\"2008-09-01\":\"2008-10-24\"]\n",
    "labels = data[[\"Sub_metering_1\", \"Sub_metering_2\", \"Sub_metering_3\"]]\n",
    "labels = labels.map(lambda x: 1 if float(x) > 0 else 0)\n",
    "data = data.drop(\n",
    "    columns=[\"Date\", \"Time\", \"Sub_metering_1\", \"Sub_metering_2\", \"Sub_metering_3\"]\n",
    ")\n",
    "data.replace(\"?\", np.nan, inplace=True)  # missing values\n",
    "data = data.astype(float)\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = 1 / 60\n",
    "results_path = \"../results/household/\" + str(sr)\n",
    "# create folders in results path\n",
    "if not os.path.exists(results_path):\n",
    "    os.makedirs(results_path + \"/mp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global_active_power\n",
      "Global_reactive_power\n",
      "Voltage\n",
      "Global_intensity\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>F_T</th>\n",
       "      <th>F_S</th>\n",
       "      <th>F_R</th>\n",
       "      <th>F_seasonal_1440</th>\n",
       "      <th>F_seasonal_10080</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Global_active_power</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.341</td>\n",
       "      <td>0.551</td>\n",
       "      <td>0.397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Global_reactive_power</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.468</td>\n",
       "      <td>0.529</td>\n",
       "      <td>0.297</td>\n",
       "      <td>0.313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Voltage</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.665</td>\n",
       "      <td>0.329</td>\n",
       "      <td>0.597</td>\n",
       "      <td>0.344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Global_intensity</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.548</td>\n",
       "      <td>0.396</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Feature    F_T    F_S    F_R  F_seasonal_1440  \\\n",
       "0    Global_active_power  0.050  0.652  0.341            0.551   \n",
       "1  Global_reactive_power  0.013  0.468  0.529            0.297   \n",
       "2                Voltage  0.063  0.665  0.329            0.597   \n",
       "3       Global_intensity  0.047  0.650  0.344            0.548   \n",
       "\n",
       "   F_seasonal_10080  \n",
       "0             0.397  \n",
       "1             0.313  \n",
       "2             0.344  \n",
       "3             0.396  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = data.columns\n",
    "stats_table = pd.DataFrame()\n",
    "resids = {}\n",
    "\n",
    "# get the data for those features\n",
    "for data_feature in features:\n",
    "    print(data_feature)\n",
    "    time_serie = data[data_feature]\n",
    "    res = MSTL(\n",
    "        time_serie, periods=[60 * 24, 60 * 24 * 7]\n",
    "    ).fit()  # seasonal period is daily and weekly\n",
    "    resids[data_feature] = res.resid\n",
    "\n",
    "    var_resid = np.var(res.resid)\n",
    "    var_observed = np.var(res.observed)\n",
    "    trend_strength = max(0, 1 - (var_resid / np.var(res.trend + res.resid)))\n",
    "    noise_strength = var_resid / var_observed\n",
    "\n",
    "    seasonal_individial_strengths = {}\n",
    "    for period in res.seasonal:\n",
    "        seasonal_individial_strengths[\"F_\" + str(period)] = max(\n",
    "            0, 1 - (var_resid / np.var(res.seasonal[period] + res.resid))\n",
    "        )\n",
    "    seasonal_strength = max(\n",
    "        0, 1 - (var_resid / np.var(res.seasonal.sum(axis=1) + res.resid))\n",
    "    )\n",
    "\n",
    "    stats_df = {\n",
    "        \"Feature\": data_feature,\n",
    "        \"F_T\": round(trend_strength, 3),\n",
    "        \"F_S\": round(seasonal_strength, 3),\n",
    "        \"F_R\": round(noise_strength, 3),\n",
    "    }\n",
    "\n",
    "    # add individual seasonal strengths to stats_df, rounded with 3 decimals\n",
    "    for period in seasonal_individial_strengths:\n",
    "        stats_df[period] = round(seasonal_individial_strengths[period], 3)\n",
    "\n",
    "    stats_table = pd.concat(\n",
    "        [stats_table, pd.DataFrame(stats_df, index=[0])], ignore_index=True\n",
    "    )\n",
    "\n",
    "pd.DataFrame(resids).to_csv(results_path + \"/resids.csv\", index=True)\n",
    "stats_table = stats_table.sort_values(by=[\"F_R\"], ascending=False)\n",
    "stats_table.to_csv(results_path + \"/decomposition_summary.csv\", index=False)\n",
    "stats_table.head().to_latex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# motif discovery\n",
    "import stumpy\n",
    "from stumpy import config\n",
    "\n",
    "config.STUMPY_EXCL_ZONE_DENOM = 2  # r = np.ceil(m/2)\n",
    "top_k_mp = 1\n",
    "include = None\n",
    "normalize = False\n",
    "subsequence_lengths = [60, 60 * 3, 60 * 6]  # 1h, 3h, 6h\n",
    "\n",
    "resids = pd.read_csv(results_path + \"/resids.csv\", index_col=0).T\n",
    "\n",
    "for m in subsequence_lengths:\n",
    "    mp, mp_indices = stumpy.mstump(resids.values, m, normalize=normalize)\n",
    "    np.save(\n",
    "        results_path\n",
    "        + \"/mp/normalized={}_topkmp={}_m={}_multivariate.npy\".format(\n",
    "            normalize, top_k_mp, m\n",
    "        ),\n",
    "        mp,\n",
    "        allow_pickle=True,\n",
    "    )\n",
    "    np.save(\n",
    "        results_path\n",
    "        + \"/mp_indices/normalized={}_topkmp={}_m={}_multivariate.npy\".format(\n",
    "            normalize, top_k_mp, m\n",
    "        ),\n",
    "        mp_indices,\n",
    "        allow_pickle=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multivar_subsequence_complexity(x):\n",
    "    # complexity for multivariate time series can be calculated as the sum of the complexity of each dimension\n",
    "    return np.sum(np.sqrt(np.sum(np.square(np.diff(x)), axis=1)))\n",
    "\n",
    "\n",
    "def table_summary_motifs(\n",
    "    motif_indices,\n",
    "    motif_distances,\n",
    "    motif_subspaces,\n",
    "    data,\n",
    "    k_distances,\n",
    "    m,\n",
    "    normalize,\n",
    "    max_allowed_dist,\n",
    "):\n",
    "    mp_stats_table = pd.DataFrame(\n",
    "        columns=[\n",
    "            \"ID\",\n",
    "            \"k_distances\",\n",
    "            \"Features\",\n",
    "            \"m\",\n",
    "            \"#Matches\",\n",
    "            \"Indices\",\n",
    "            \"max(dists)\",\n",
    "            \"min(dists)\",\n",
    "            \"med(dists)\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    motif_index = 0\n",
    "\n",
    "    n_vars, n_time = data.shape\n",
    "\n",
    "    if normalize:\n",
    "        data = (data - np.mean(data, axis=1)[:, np.newaxis]) / np.std(data, axis=1)[\n",
    "            :, np.newaxis\n",
    "        ]\n",
    "\n",
    "    dtypes = [float] * len(data)\n",
    "    model_empirical = NullModel(data, dtypes, model=\"empirical\")\n",
    "\n",
    "    for motif_indice, match_indices in enumerate(motif_indices):\n",
    "        dimensions = motif_subspaces[motif_indice]\n",
    "\n",
    "        # remove filling values of -1 and Nans from motif_indices and match_distances\n",
    "        match_indices = match_indices[match_indices != -1]\n",
    "        match_distances = motif_distances[motif_indice]\n",
    "        match_distances = match_distances[~np.isnan(match_distances)]\n",
    "\n",
    "        # if is empty, skip\n",
    "        if len(match_indices) == 0:\n",
    "            continue\n",
    "\n",
    "        excl_zone = np.ceil(m / config.STUMPY_EXCL_ZONE_DENOM)\n",
    "\n",
    "        # remove trivial matches\n",
    "        non_trivial_matches = []\n",
    "        for indice in match_indices:\n",
    "            trivial = False\n",
    "            for indice_new in non_trivial_matches:\n",
    "                if abs(indice - indice_new) <= excl_zone:\n",
    "                    trivial = True\n",
    "                    break\n",
    "            if not trivial:\n",
    "                non_trivial_matches.append(indice)\n",
    "        match_indices = non_trivial_matches\n",
    "\n",
    "        max_possible_matches = int(np.floor((n_time - m) / excl_zone + 1))\n",
    "\n",
    "        # get the multidim time serie motif in the dimensions\n",
    "        multivar_subsequence = data[dimensions][\n",
    "            :, match_indices[0] : match_indices[0] + m\n",
    "        ]\n",
    "\n",
    "        # minmax normalize subsequence\n",
    "        epsilon = 1e-10  # to avoid division by zero\n",
    "        min_values = multivar_subsequence.min(axis=1, keepdims=True)\n",
    "        max_values = multivar_subsequence.max(axis=1, keepdims=True)\n",
    "        normalized_multivar_subsequence = (multivar_subsequence - min_values) / (\n",
    "            max_values - min_values + epsilon\n",
    "        )\n",
    "        ce_norm_subsequence = multivar_subsequence_complexity(\n",
    "            normalized_multivar_subsequence\n",
    "        )\n",
    "        norm_ce_norm_subsequence = ce_norm_subsequence / (\n",
    "            np.sqrt(len(multivar_subsequence[0]) - 1) * len(dimensions)\n",
    "        )\n",
    "\n",
    "        max_dist = np.max(match_distances)\n",
    "        min_dist = np.min(match_distances[1:])\n",
    "\n",
    "        if k_distances is None:  # consider all matches\n",
    "            med_dist = np.median(match_distances[1:])\n",
    "        else:  # consider only the k closest matches\n",
    "            med_dist = np.median(match_distances[1 : k_distances + 1])\n",
    "\n",
    "        # np.nanmax([np.nanmean(D) - 2.0 * np.nanstd(D), np.nanmin(D)])\n",
    "        if max_allowed_dist is None:\n",
    "            # D The distance profile of `Q` with `T`. It is a 1D numpy array of size\n",
    "            # `len(T)-len(Q)+1`, where `D[i]` is the distance between query `Q` and\n",
    "            # `T[i : i + len(Q)]`\n",
    "            D = np.empty((n_vars, n_time - m + 1))\n",
    "            for i in range(n_vars):\n",
    "                D[i, :] = stumpy.mass(\n",
    "                    multivar_subsequence[i], data[i], normalize=normalize\n",
    "                )\n",
    "            D = np.mean(D, axis=0)\n",
    "            D_copy = D.copy().astype(np.float64)\n",
    "            D_copy[np.isinf(D_copy)] = np.nan\n",
    "            motif_max_allowed_dist = np.nanmax(\n",
    "                [np.nanmean(D_copy) - 2.0 * np.nanstd(D_copy), np.nanmin(D_copy)]\n",
    "            )\n",
    "        else:\n",
    "            motif_max_allowed_dist = max_allowed_dist\n",
    "\n",
    "        unified_weights = \"0.33,0.33,0.33\"\n",
    "        w1, w2, w3 = map(float, unified_weights.split(\",\"))\n",
    "        unified = (\n",
    "            w1 * (1 - (med_dist / motif_max_allowed_dist))\n",
    "            + w2 * (len(match_indices) / max_possible_matches)\n",
    "            + w3 * norm_ce_norm_subsequence\n",
    "        )\n",
    "\n",
    "        # remove timepoints from time series in match all indices + m\n",
    "        time_series_nomatches = data.copy()\n",
    "        # list of indexes to remove\n",
    "        indexes_to_remove = [\n",
    "            i for index in match_indices for i in range(index, index + m)\n",
    "        ]\n",
    "        # put zero in the indexes to remove\n",
    "        time_series_nomatches[:, indexes_to_remove] = 0\n",
    "\n",
    "        # calculate variance explained by the motif\n",
    "        vars_explained = []\n",
    "        for i in range(len(dimensions)):\n",
    "            vars_explained.append(\n",
    "                100\n",
    "                * (\n",
    "                    1\n",
    "                    - (\n",
    "                        np.mean(np.abs(time_series_nomatches[i]))\n",
    "                        / np.mean(np.abs(data[i]))\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "        variance_explained = np.mean(vars_explained)\n",
    "\n",
    "        # data features are now the ones in the dimensions\n",
    "        used_features = [f\"{dimension}\" for dimension in dimensions]\n",
    "\n",
    "        # max_delta = motif_max_allowed_dist # (worst case) max_dist = sqrt(max_delta^2) <=> max_delta = max_dist\n",
    "        max_delta = math.sqrt(motif_max_allowed_dist**2 / m)\n",
    "        delta_thresholds = [max_delta] * len(data)\n",
    "\n",
    "        #########SIG#########\n",
    "        motif = Motif(\n",
    "            multivar_subsequence, dimensions, delta_thresholds, len(match_indices)\n",
    "        )\n",
    "        p = motif.set_pattern_probability(model_empirical, vars_indep=True)\n",
    "        pvalue = motif.set_significance(\n",
    "            max_possible_matches, n_vars, idd_correction=False\n",
    "        )\n",
    "\n",
    "        stats_df = {\n",
    "            \"ID\": str(motif_index),\n",
    "            \"k\": len(dimensions),\n",
    "            \"Features\": \",\".join(used_features),\n",
    "            \"m\": m,\n",
    "            \"#Matches\": len(match_indices) - 1,\n",
    "            \"Indices\": match_indices,\n",
    "            \"max(dists)\": np.around(max_dist, 3),\n",
    "            \"min(dists)\": np.around(min_dist, 3),\n",
    "            \"med(dists)\": np.around(med_dist, 3),\n",
    "            \"CE\": np.around(norm_ce_norm_subsequence, 3),\n",
    "            \"Score Unified\": np.around(unified, 3),\n",
    "            \"Explained Var(%)\": np.around(variance_explained, 2),\n",
    "            \"P\": p,\n",
    "            \"p-value\": pvalue,\n",
    "        }\n",
    "\n",
    "        mp_stats_table = (\n",
    "            pd.DataFrame.from_records([stats_df])\n",
    "            if mp_stats_table.empty\n",
    "            else pd.concat(\n",
    "                [mp_stats_table, pd.DataFrame.from_records([stats_df])],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        motif_index += 1\n",
    "    return mp_stats_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_distances = None\n",
    "min_neighbors = 2\n",
    "cutoff = np.inf\n",
    "max_matches = 99999\n",
    "max_distance = None\n",
    "max_motifs = 99999\n",
    "k = None\n",
    "mp_stats_table = pd.DataFrame()\n",
    "for m in subsequence_lengths:\n",
    "    X = resids.values\n",
    "    mp = np.load(\n",
    "        results_path\n",
    "        + \"/mp/normalized={}_topkmp={}_m={}_multivariate.npy\".format(\n",
    "            normalize, top_k_mp, m\n",
    "        ),\n",
    "        allow_pickle=True,\n",
    "    )\n",
    "    mp_indices = np.load(\n",
    "        results_path\n",
    "        + \"/mp_indices/normalized={}_topkmp={}_m={}_multivariate.npy\".format(\n",
    "            normalize, top_k_mp, m\n",
    "        ),\n",
    "        allow_pickle=True,\n",
    "    )\n",
    "\n",
    "    motif_distances, motif_indices, motif_subspaces, motif_mdls = stumpy.mmotifs(\n",
    "        X,\n",
    "        mp,\n",
    "        mp_indices,\n",
    "        min_neighbors=min_neighbors,\n",
    "        max_distance=max_distance,\n",
    "        cutoffs=np.inf,\n",
    "        max_matches=max_matches,\n",
    "        max_motifs=max_motifs,\n",
    "        k=k,\n",
    "        include=include,\n",
    "        normalize=normalize,\n",
    "    )\n",
    "\n",
    "    if len(motif_indices[0]) == 0:\n",
    "        continue\n",
    "    print(\"m:{}, #Motifs:{}\".format(m, len(motif_indices)))\n",
    "    table = table_summary_motifs(\n",
    "        motif_indices, motif_distances, motif_subspaces, X, m, normalize, max_distance\n",
    "    )\n",
    "    print(\"Sig \", np.sum(table[\"p-value\"] < 0.001))\n",
    "    # hochberg procedure\n",
    "    p_values = table[\"p-value\"].to_numpy()\n",
    "    critical_value = NullModel.hochberg_critical_value(p_values, 0.05)\n",
    "    sig = (\n",
    "        table[\"p-value\"] < critical_value\n",
    "        if critical_value != 0\n",
    "        else table[\"p-value\"] <= critical_value\n",
    "    )\n",
    "    table[\"Sig_Hochber\"] = sig\n",
    "\n",
    "    print(\n",
    "        \"Sig after Hochberg: {}, critical value: {}\".format(np.sum(sig), critical_value)\n",
    "    )\n",
    "    mp_stats_table = (\n",
    "        table\n",
    "        if mp_stats_table.empty\n",
    "        else pd.concat([mp_stats_table, table], ignore_index=True)\n",
    "    )\n",
    "\n",
    "    mp_stats_table.to_csv(\n",
    "        results_path\n",
    "        + \"/table_motifs_normalize={}_min_neighbors={}_max_distance={}_cutoff={}_max_matches={}_max_motifs={}.csv\".format(\n",
    "            normalize, min_neighbors, max_distance, cutoff, max_matches, max_motifs\n",
    "        ),\n",
    "        index=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read motifs table\n",
    "mp_stats_table = pd.read_csv(\n",
    "    results_path\n",
    "    + \"/table_motifs_normalize={}_min_neighbors={}_max_distance={}_cutoff={}_max_matches={}_max_motifs={}.csv\".format(\n",
    "        normalize, min_neighbors, max_distance, cutoff, max_matches, max_motifs\n",
    "    )\n",
    ")\n",
    "\n",
    "# for motif in mp_stats_table\n",
    "indexes_to_remove = set()\n",
    "for index, motif in mp_stats_table.iterrows():\n",
    "    indexes_to_remove.update(\n",
    "        [\n",
    "            i\n",
    "            for index in eval(motif[\"Indices\"])\n",
    "            for i in range(int(index), int(index) + int(motif[\"m\"]))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "datetime_indexes_to_remove = resids.T.index[sorted(indexes_to_remove)]\n",
    "\n",
    "residual_nomatches = resids.T.copy()\n",
    "for feature in residual_nomatches.columns:\n",
    "    residual_nomatches.loc[datetime_indexes_to_remove, feature] = 0\n",
    "    variance_explained = 100 * (\n",
    "        1\n",
    "        - (\n",
    "            np.mean(np.abs(residual_nomatches[feature]))\n",
    "            / np.mean(np.abs(resids.T[feature]))\n",
    "        )\n",
    "    )\n",
    "    print(f\"{feature}: {variance_explained}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new table for each motif length with statistics of the motifs (number of motifs found,\n",
    "# number of significant motifs, average number of matches +- std, average of features +- std,\n",
    "# average probability +- std, average pvalue +- std)\n",
    "\n",
    "mp_stats_table = pd.read_csv(\n",
    "    results_path\n",
    "    + \"/table_motifs_normalize={}_min_neighbors={}_max_distance={}_cutoff={}_max_matches={}_max_motifs={}.csv\".format(\n",
    "        normalize, min_neighbors, max_distance, cutoff, max_matches, max_motifs\n",
    "    )\n",
    ")\n",
    "motif_lengths = mp_stats_table[\"m\"].unique()\n",
    "motif_stats_table = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"m\",\n",
    "        \"#motifs\",\n",
    "        \"avg_n_matches\",\n",
    "        \"avg_n_features\",\n",
    "        \"avg_probability\",\n",
    "        \"avg_pvalue\",\n",
    "        \"#sig_motifs(<0.01)\",\n",
    "        \"significant\",\n",
    "        \"#sig_hochberg\",\n",
    "    ]\n",
    ")\n",
    "for m in subsequence_lengths:\n",
    "    table = mp_stats_table[mp_stats_table[\"m\"] == m]\n",
    "    if table.empty:\n",
    "        continue\n",
    "    n_motifs = table.shape[0]\n",
    "    n_sig_motifs_0001 = table[table[\"p-value\"] < 0.001].shape[0]\n",
    "    n_sig_motifs_hochberg = table[table[\"Sig_Hochber\"]].shape[0]\n",
    "    avg_n_matches = (\n",
    "        round(table[\"#Matches\"].mean(), 2),\n",
    "        round(table[\"#Matches\"].std(), 3),\n",
    "    )\n",
    "    avg_n_features = round(table[\"k\"].mean(), 2), round(table[\"k\"].std(), 3)\n",
    "    avg_probability = table[\"P\"].mean(), table[\"P\"].std()\n",
    "    avg_pvalue = table[\"p-value\"].mean(), table[\"p-value\"].std()\n",
    "\n",
    "    stats_df = {\n",
    "        \"m\": m,\n",
    "        \"#motifs\": n_motifs,\n",
    "        \"#sig_motifs(<0.001)\": n_sig_motifs_0001,\n",
    "        \"significant\": (n_sig_motifs_0001 * 100) / n_motifs,\n",
    "        \"avg_n_matches\": avg_n_matches,\n",
    "        \"avg_n_features\": avg_n_features,\n",
    "    }\n",
    "\n",
    "    motif_stats_table = (\n",
    "        pd.DataFrame.from_records([stats_df])\n",
    "        if motif_stats_table.empty\n",
    "        else pd.concat(\n",
    "            [motif_stats_table, pd.DataFrame.from_records([stats_df])],\n",
    "            ignore_index=True,\n",
    "        )\n",
    "    )\n",
    "print(motif_stats_table.to_latex(index=False, float_format=\"%.3f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_stats_table = pd.read_csv(\n",
    "    results_path\n",
    "    + \"/table_motifs_normalize={}_min_neighbors={}_max_distance={}_cutoff={}_max_matches={}_max_motifs={}.csv\".format(\n",
    "        normalize, min_neighbors, max_distance, cutoff, max_matches, max_motifs\n",
    "    )\n",
    ")\n",
    "# excluded p-value > 0.001\n",
    "mp_stats_table = mp_stats_table[mp_stats_table[\"p-value\"] < 0.001]\n",
    "subsequence_lengths = mp_stats_table[\"m\"].unique()\n",
    "for m in subsequence_lengths:\n",
    "    print(\"########## m:{} #########\".format(m))\n",
    "    top_motifs = mp_stats_table[mp_stats_table[\"m\"] == m]\n",
    "    top_motifs = top_motifs.sort_values(by=\"Score Unified\", ascending=False).head(5)\n",
    "    top_motifs = top_motifs[\n",
    "        [\n",
    "            \"ID\",\n",
    "            \"#Matches\",\n",
    "            \"CE\",\n",
    "            \"Score Unified\",\n",
    "            \"max(dists)\",\n",
    "            \"min(dists)\",\n",
    "            \"med(dists)\",\n",
    "            \"p-value\",\n",
    "            \"Explained Var(%)\",\n",
    "        ]\n",
    "    ]\n",
    "    top_motifs[\"p-value\"] = top_motifs[\"p-value\"].apply(lambda x: f\"{x:.2e}\")\n",
    "    print(top_motifs.to_latex(index=False, float_format=\"%.3f\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_motif(ts_list, features, m, motif_indexes, motif_name):\n",
    "    fig, axes = plt.subplots(\n",
    "        ncols=2, nrows=len(ts_list), figsize=(10, 2 * len(ts_list)), squeeze=False\n",
    "    )\n",
    "    for i in range(0, len(ts_list)):\n",
    "        ts = ts_list[i]\n",
    "        # plot light grey\n",
    "        axes[i, 1].plot(ts, color=\"black\", linewidth=0.5, alpha=0.5)\n",
    "\n",
    "        colors = plt.cm.tab20(np.linspace(0, 1, len(motif_indexes)))\n",
    "        axes[i, 0].set_prop_cycle(\"color\", colors)\n",
    "        axes[i, 1].set_prop_cycle(\"color\", colors)\n",
    "\n",
    "        for index in motif_indexes:\n",
    "            subsequence_match = ts.iloc[index : index + m]\n",
    "            # original motif in the next plot with the same color\n",
    "            axes[i, 0].plot(subsequence_match.values)\n",
    "            # highlight the motif in the original time serie\n",
    "            axes[i, 1].plot(subsequence_match, linewidth=2)\n",
    "\n",
    "        plt.setp(axes[i, 0].xaxis.get_majorticklabels(), rotation=90)\n",
    "        # remove x labels and ticks except from last plot\n",
    "        if i != len(ts_list) - 1:\n",
    "            axes[i, 0].axes.get_xaxis().set_visible(False)\n",
    "            axes[i, 1].axes.get_xaxis().set_visible(False)\n",
    "\n",
    "        # label x with i+index\n",
    "        plt.setp(axes[i, 0].xaxis.get_majorticklabels(), rotation=90)\n",
    "\n",
    "        # format the x axis to show the time and rotate for better reading\n",
    "        axes[i, 1].xaxis.set_major_formatter(mdates.DateFormatter(\"%H:%M:%S\"))\n",
    "        plt.setp(axes[i, 1].xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "        axes[i, 0].set_ylabel(features[i], rotation=90, size=\"large\")\n",
    "\n",
    "    # title of the fig\n",
    "    axes[0, 0].set_title(\"Raw Subsequences\")\n",
    "    axes[0, 1].set_title(\"Motif in TS\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        results_path + \"/m=\" + str(m) + \"_motif_\" + str(motif_name) + \".pdf\",\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot top motif\n",
    "mp_stats_table = pd.read_csv(\n",
    "    results_path\n",
    "    + \"/table_motifs_normalize={}_min_neighbors={}_max_distance={}_cutoff={}_max_matches={}_max_motifs={}.csv\".format(\n",
    "        normalize, min_neighbors, max_distance, cutoff, max_matches, max_motifs\n",
    "    )\n",
    ")\n",
    "subsequence_lengths = mp_stats_table[\"m\"].unique()\n",
    "ts = resids\n",
    "for m in subsequence_lengths:\n",
    "    print(\"Motif length: \", m)\n",
    "    top_motifs = mp_stats_table[mp_stats_table[\"m\"] == m]\n",
    "    for top_motif in top_motifs.to_dict(orient=\"records\"):\n",
    "        m = top_motif[\"m\"]\n",
    "        dimensions = top_motif[\"Features\"].split(\",\")\n",
    "        dimensions = sorted([int(dimension) for dimension in dimensions])\n",
    "        features = [resids.T.columns[dimension] for dimension in dimensions]\n",
    "        indices = top_motif[\"Indices\"].replace(\"[\", \"\").replace(\"]\", \"\").split(\",\")\n",
    "        indices = [int(i) for i in indices]\n",
    "        motif_name = top_motif[\"ID\"]\n",
    "        ts_list = [resids.T[feature] for feature in features]\n",
    "        ts_list.append(labels)\n",
    "        features.append(\"label\")\n",
    "        plot_motif(ts_list, features, m, indices, motif_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "motifsinresidualenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
