{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ3. Multidimensional motif discovery (RQ3)\n",
    "\n",
    "We conduct a guided multidimensional motif search on the multivariate residual series derived from the São Domingos de Benfica (Estrada Luz $|$ Oeste) TAZ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pylab as pylab\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.seasonal import MSTL, STL\n",
    "\n",
    "from msig import Motif, NullModel\n",
    "\n",
    "params = {\"legend.fontsize\": \"xx-large\", \"axes.labelsize\": 20}\n",
    "pylab.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>taz_id</th>\n",
       "      <th>taz_name</th>\n",
       "      <th>one_time</th>\n",
       "      <th>sum_terminals</th>\n",
       "      <th>sum_roaming_terminals</th>\n",
       "      <th>sum_terminals_with_active_data</th>\n",
       "      <th>sum_roaming_terminals_with_active_data</th>\n",
       "      <th>sum_phonecalls</th>\n",
       "      <th>sum_ended_phonecalls</th>\n",
       "      <th>avg_upstream_bandwidth</th>\n",
       "      <th>avg_downstream_bandwidth</th>\n",
       "      <th>min_avg_downstream_bandwidth</th>\n",
       "      <th>min_avg_upstream_bandwidth</th>\n",
       "      <th>max_upstream_bandwidth</th>\n",
       "      <th>max_downstream_bandwidth</th>\n",
       "      <th>sum_sum_terminals_sharing</th>\n",
       "      <th>wkt_taz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69</td>\n",
       "      <td>Areeiro (Alto Pina)</td>\n",
       "      <td>2021-11-26 09:00:00</td>\n",
       "      <td>61599.0</td>\n",
       "      <td>1131.0</td>\n",
       "      <td>58113.0</td>\n",
       "      <td>1107.0</td>\n",
       "      <td>2896.0</td>\n",
       "      <td>574.0</td>\n",
       "      <td>7524.772105</td>\n",
       "      <td>101178.976512</td>\n",
       "      <td>104.328000</td>\n",
       "      <td>105.621997</td>\n",
       "      <td>39330404.0</td>\n",
       "      <td>135947840.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>POLYGON ((-9.131441060130475 38.74471703960398...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>70</td>\n",
       "      <td>Arroios (Estefânia)</td>\n",
       "      <td>2021-11-26 09:00:00</td>\n",
       "      <td>145078.0</td>\n",
       "      <td>4477.0</td>\n",
       "      <td>138532.0</td>\n",
       "      <td>4351.0</td>\n",
       "      <td>5149.0</td>\n",
       "      <td>1178.0</td>\n",
       "      <td>7678.938176</td>\n",
       "      <td>53412.218959</td>\n",
       "      <td>2752.320035</td>\n",
       "      <td>14550.430893</td>\n",
       "      <td>63988960.0</td>\n",
       "      <td>405100896.0</td>\n",
       "      <td>12.43</td>\n",
       "      <td>POLYGON ((-9.144729944918017 38.73514419538672...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>71</td>\n",
       "      <td>Arroios (Arroios | Norte)</td>\n",
       "      <td>2021-11-26 09:00:00</td>\n",
       "      <td>31194.0</td>\n",
       "      <td>945.0</td>\n",
       "      <td>29608.0</td>\n",
       "      <td>920.0</td>\n",
       "      <td>1284.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>5017.655536</td>\n",
       "      <td>58454.086946</td>\n",
       "      <td>943.014966</td>\n",
       "      <td>842.953344</td>\n",
       "      <td>3146271.0</td>\n",
       "      <td>142608816.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>POLYGON ((-9.136504202275457 38.73689574768296...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Alvalade (Campo Grande)</td>\n",
       "      <td>2021-11-26 09:00:00</td>\n",
       "      <td>51055.0</td>\n",
       "      <td>864.0</td>\n",
       "      <td>48474.0</td>\n",
       "      <td>849.0</td>\n",
       "      <td>1520.0</td>\n",
       "      <td>474.0</td>\n",
       "      <td>8951.088582</td>\n",
       "      <td>70079.739010</td>\n",
       "      <td>2236.359946</td>\n",
       "      <td>17768.382601</td>\n",
       "      <td>33589812.0</td>\n",
       "      <td>96555216.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>POLYGON ((-9.15660999782037 38.7593274576093, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40</td>\n",
       "      <td>Lumiar (Telheiras | Oeste)</td>\n",
       "      <td>2021-11-26 09:00:00</td>\n",
       "      <td>40523.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>37014.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>2314.0</td>\n",
       "      <td>382.0</td>\n",
       "      <td>3442.786203</td>\n",
       "      <td>32570.243025</td>\n",
       "      <td>1.238571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16425652.0</td>\n",
       "      <td>168218912.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>POLYGON ((-9.17712280774748 38.76422000124379,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175218</th>\n",
       "      <td>16</td>\n",
       "      <td>Estrela (Estrela)</td>\n",
       "      <td>2021-11-26 09:00:00</td>\n",
       "      <td>199615.0</td>\n",
       "      <td>6739.0</td>\n",
       "      <td>190222.0</td>\n",
       "      <td>6497.0</td>\n",
       "      <td>7990.0</td>\n",
       "      <td>1839.0</td>\n",
       "      <td>10347.174511</td>\n",
       "      <td>75677.619332</td>\n",
       "      <td>573.914546</td>\n",
       "      <td>628.886265</td>\n",
       "      <td>67788656.0</td>\n",
       "      <td>566004672.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>POLYGON ((-9.16455004654274 38.71408737534585,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175219</th>\n",
       "      <td>12</td>\n",
       "      <td>Ajuda (Ajuda | Norte)</td>\n",
       "      <td>2021-11-26 09:00:00</td>\n",
       "      <td>27901.0</td>\n",
       "      <td>487.0</td>\n",
       "      <td>25890.0</td>\n",
       "      <td>481.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>342.0</td>\n",
       "      <td>199281.838168</td>\n",
       "      <td>236425.820611</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>130461056.0</td>\n",
       "      <td>152884192.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>POLYGON ((-9.194104152143993 38.72438283212201...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175220</th>\n",
       "      <td>15</td>\n",
       "      <td>Estrela (Lapa)</td>\n",
       "      <td>2021-11-26 09:00:00</td>\n",
       "      <td>9932.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>9298.0</td>\n",
       "      <td>429.0</td>\n",
       "      <td>605.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>3801.634144</td>\n",
       "      <td>39603.158490</td>\n",
       "      <td>1.328000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1539097.0</td>\n",
       "      <td>67348640.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>POLYGON ((-9.167543700000001 38.70657329999994...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175221</th>\n",
       "      <td>13</td>\n",
       "      <td>Estrela (Ribeirinha - Belém)</td>\n",
       "      <td>2021-11-26 09:00:00</td>\n",
       "      <td>6586.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>6119.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>410.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>2781.907062</td>\n",
       "      <td>16101.249632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11564562.0</td>\n",
       "      <td>18501752.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>POLYGON ((-9.161911000000002 38.70242249999994...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175222</th>\n",
       "      <td>66</td>\n",
       "      <td>Avenidas Novas (Avenidas Novas | Este)</td>\n",
       "      <td>2021-11-26 09:00:00</td>\n",
       "      <td>150273.0</td>\n",
       "      <td>2975.0</td>\n",
       "      <td>142580.0</td>\n",
       "      <td>2910.0</td>\n",
       "      <td>6224.0</td>\n",
       "      <td>1741.0</td>\n",
       "      <td>9989.279638</td>\n",
       "      <td>61916.219515</td>\n",
       "      <td>2348.386035</td>\n",
       "      <td>11053.150286</td>\n",
       "      <td>61554624.0</td>\n",
       "      <td>624569088.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>POLYGON ((-9.149802831787737 38.74050549525003...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>171527 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        taz_id                                taz_name            one_time  \\\n",
       "0           69                     Areeiro (Alto Pina) 2021-11-26 09:00:00   \n",
       "1           70                     Arroios (Estefânia) 2021-11-26 09:00:00   \n",
       "2           71               Arroios (Arroios | Norte) 2021-11-26 09:00:00   \n",
       "3           53                 Alvalade (Campo Grande) 2021-11-26 09:00:00   \n",
       "4           40              Lumiar (Telheiras | Oeste) 2021-11-26 09:00:00   \n",
       "...        ...                                     ...                 ...   \n",
       "175218      16                       Estrela (Estrela) 2021-11-26 09:00:00   \n",
       "175219      12                   Ajuda (Ajuda | Norte) 2021-11-26 09:00:00   \n",
       "175220      15                          Estrela (Lapa) 2021-11-26 09:00:00   \n",
       "175221      13            Estrela (Ribeirinha - Belém) 2021-11-26 09:00:00   \n",
       "175222      66  Avenidas Novas (Avenidas Novas | Este) 2021-11-26 09:00:00   \n",
       "\n",
       "        sum_terminals  sum_roaming_terminals  sum_terminals_with_active_data  \\\n",
       "0             61599.0                 1131.0                         58113.0   \n",
       "1            145078.0                 4477.0                        138532.0   \n",
       "2             31194.0                  945.0                         29608.0   \n",
       "3             51055.0                  864.0                         48474.0   \n",
       "4             40523.0                  255.0                         37014.0   \n",
       "...               ...                    ...                             ...   \n",
       "175218       199615.0                 6739.0                        190222.0   \n",
       "175219        27901.0                  487.0                         25890.0   \n",
       "175220         9932.0                  432.0                          9298.0   \n",
       "175221         6586.0                  138.0                          6119.0   \n",
       "175222       150273.0                 2975.0                        142580.0   \n",
       "\n",
       "        sum_roaming_terminals_with_active_data  sum_phonecalls  \\\n",
       "0                                       1107.0          2896.0   \n",
       "1                                       4351.0          5149.0   \n",
       "2                                        920.0          1284.0   \n",
       "3                                        849.0          1520.0   \n",
       "4                                        252.0          2314.0   \n",
       "...                                        ...             ...   \n",
       "175218                                  6497.0          7990.0   \n",
       "175219                                   481.0          1100.0   \n",
       "175220                                   429.0           605.0   \n",
       "175221                                   138.0           410.0   \n",
       "175222                                  2910.0          6224.0   \n",
       "\n",
       "        sum_ended_phonecalls  avg_upstream_bandwidth  \\\n",
       "0                      574.0             7524.772105   \n",
       "1                     1178.0             7678.938176   \n",
       "2                      420.0             5017.655536   \n",
       "3                      474.0             8951.088582   \n",
       "4                      382.0             3442.786203   \n",
       "...                      ...                     ...   \n",
       "175218                1839.0            10347.174511   \n",
       "175219                 342.0           199281.838168   \n",
       "175220                 154.0             3801.634144   \n",
       "175221                  77.0             2781.907062   \n",
       "175222                1741.0             9989.279638   \n",
       "\n",
       "        avg_downstream_bandwidth  min_avg_downstream_bandwidth  \\\n",
       "0                  101178.976512                    104.328000   \n",
       "1                   53412.218959                   2752.320035   \n",
       "2                   58454.086946                    943.014966   \n",
       "3                   70079.739010                   2236.359946   \n",
       "4                   32570.243025                      1.238571   \n",
       "...                          ...                           ...   \n",
       "175218              75677.619332                    573.914546   \n",
       "175219             236425.820611                      0.000000   \n",
       "175220              39603.158490                      1.328000   \n",
       "175221              16101.249632                      0.000000   \n",
       "175222              61916.219515                   2348.386035   \n",
       "\n",
       "        min_avg_upstream_bandwidth  max_upstream_bandwidth  \\\n",
       "0                       105.621997              39330404.0   \n",
       "1                     14550.430893              63988960.0   \n",
       "2                       842.953344               3146271.0   \n",
       "3                     17768.382601              33589812.0   \n",
       "4                         0.000000              16425652.0   \n",
       "...                            ...                     ...   \n",
       "175218                  628.886265              67788656.0   \n",
       "175219                    0.000000             130461056.0   \n",
       "175220                    0.000000               1539097.0   \n",
       "175221                    0.000000              11564562.0   \n",
       "175222                11053.150286              61554624.0   \n",
       "\n",
       "        max_downstream_bandwidth  sum_sum_terminals_sharing  \\\n",
       "0                    135947840.0                       0.00   \n",
       "1                    405100896.0                      12.43   \n",
       "2                    142608816.0                       0.00   \n",
       "3                     96555216.0                       0.00   \n",
       "4                    168218912.0                       0.00   \n",
       "...                          ...                        ...   \n",
       "175218               566004672.0                       0.00   \n",
       "175219               152884192.0                       0.00   \n",
       "175220                67348640.0                       0.00   \n",
       "175221                18501752.0                       0.00   \n",
       "175222               624569088.0                       0.00   \n",
       "\n",
       "                                                  wkt_taz  \n",
       "0       POLYGON ((-9.131441060130475 38.74471703960398...  \n",
       "1       POLYGON ((-9.144729944918017 38.73514419538672...  \n",
       "2       POLYGON ((-9.136504202275457 38.73689574768296...  \n",
       "3       POLYGON ((-9.15660999782037 38.7593274576093, ...  \n",
       "4       POLYGON ((-9.17712280774748 38.76422000124379,...  \n",
       "...                                                   ...  \n",
       "175218  POLYGON ((-9.16455004654274 38.71408737534585,...  \n",
       "175219  POLYGON ((-9.194104152143993 38.72438283212201...  \n",
       "175220  POLYGON ((-9.167543700000001 38.70657329999994...  \n",
       "175221  POLYGON ((-9.161911000000002 38.70242249999994...  \n",
       "175222  POLYGON ((-9.149802831787737 38.74050549525003...  \n",
       "\n",
       "[171527 rows x 17 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read csv\n",
    "hourly_taz_data = pd.read_csv(\n",
    "    \"../data/populationdensity/hourly_taz.csv\",\n",
    "    parse_dates=[\"one_time\"],\n",
    "    date_format=\"%Y-%m-%d %H:%M:%S\",\n",
    "    index_col=0,\n",
    ")\n",
    "# taz_id to taz_name dict\n",
    "taz_id_name = {}\n",
    "for taz_id in hourly_taz_data[\"taz_id\"].unique():\n",
    "    taz_name = hourly_taz_data[hourly_taz_data[\"taz_id\"] == taz_id][\"taz_name\"].values[\n",
    "        0\n",
    "    ]\n",
    "    taz_id_name[taz_id] = taz_name\n",
    "\n",
    "hourly_taz_data = hourly_taz_data[hourly_taz_data[\"taz_name\"] != \"Beato (Picheleira)\"]\n",
    "hourly_taz_data = hourly_taz_data[\n",
    "    hourly_taz_data[\"taz_name\"] != \"Alcântara (Ribeirinha - Belém)\"\n",
    "]\n",
    "hourly_taz_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = \"../results/populationdensity/rq3/hourly_taz\"\n",
    "# create folders in results path\n",
    "if not os.path.exists(results_path):\n",
    "    os.makedirs(results_path + \"/mp\")\n",
    "    os.makedirs(results_path + \"/indices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Name</th>\n",
       "      <th>Feature</th>\n",
       "      <th>F_T</th>\n",
       "      <th>F_S</th>\n",
       "      <th>F_R</th>\n",
       "      <th>F_seasonal_24</th>\n",
       "      <th>F_seasonal_168</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>São Domingos de Benfica (Estrada Luz | Oeste)</td>\n",
       "      <td>sum_roaming_terminals</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.614</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37</td>\n",
       "      <td>São Domingos de Benfica (Estrada Luz | Oeste)</td>\n",
       "      <td>sum_terminals</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.574</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37</td>\n",
       "      <td>São Domingos de Benfica (Estrada Luz | Oeste)</td>\n",
       "      <td>min_avg_upstream_bandwidth</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.477</td>\n",
       "      <td>0.479</td>\n",
       "      <td>0.322</td>\n",
       "      <td>0.299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37</td>\n",
       "      <td>São Domingos de Benfica (Estrada Luz | Oeste)</td>\n",
       "      <td>sum_ended_phonecalls</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.586</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.511</td>\n",
       "      <td>0.266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>São Domingos de Benfica (Estrada Luz | Oeste)</td>\n",
       "      <td>sum_phonecalls</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.639</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.557</td>\n",
       "      <td>0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>37</td>\n",
       "      <td>São Domingos de Benfica (Estrada Luz | Oeste)</td>\n",
       "      <td>min_avg_downstream_bandwidth</td>\n",
       "      <td>0.532</td>\n",
       "      <td>0.596</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.343</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                           Name  \\\n",
       "1  37  São Domingos de Benfica (Estrada Luz | Oeste)   \n",
       "0  37  São Domingos de Benfica (Estrada Luz | Oeste)   \n",
       "4  37  São Domingos de Benfica (Estrada Luz | Oeste)   \n",
       "3  37  São Domingos de Benfica (Estrada Luz | Oeste)   \n",
       "2  37  São Domingos de Benfica (Estrada Luz | Oeste)   \n",
       "5  37  São Domingos de Benfica (Estrada Luz | Oeste)   \n",
       "\n",
       "                        Feature    F_T    F_S    F_R  F_seasonal_24  \\\n",
       "1         sum_roaming_terminals  0.053  0.364  0.614          0.229   \n",
       "0                 sum_terminals  0.059  0.404  0.574          0.267   \n",
       "4    min_avg_upstream_bandwidth  0.157  0.477  0.479          0.322   \n",
       "3          sum_ended_phonecalls  0.112  0.586  0.394          0.511   \n",
       "2                sum_phonecalls  0.122  0.639  0.343          0.557   \n",
       "5  min_avg_downstream_bandwidth  0.532  0.596  0.280          0.485   \n",
       "\n",
       "   F_seasonal_168  \n",
       "1           0.216  \n",
       "0           0.236  \n",
       "4           0.299  \n",
       "3           0.266  \n",
       "2           0.325  \n",
       "5           0.343  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = [\n",
    "    \"sum_terminals\",\n",
    "    \"sum_roaming_terminals\",\n",
    "    \"sum_phonecalls\",\n",
    "    \"sum_ended_phonecalls\",\n",
    "    \"min_avg_upstream_bandwidth\",\n",
    "    \"min_avg_downstream_bandwidth\",\n",
    "]\n",
    "stats_table = pd.DataFrame()\n",
    "resids = {}\n",
    "\n",
    "# get the data for São Domingos de Benfica (Estrada Luz | Oeste)\n",
    "taz_data = hourly_taz_data[\n",
    "    hourly_taz_data[\"taz_name\"] == \"São Domingos de Benfica (Estrada Luz | Oeste)\"\n",
    "]\n",
    "taz_data = taz_data.drop(columns=[\"taz_name\"])\n",
    "taz_data[\"datetime\"] = pd.to_datetime(taz_data[\"one_time\"])\n",
    "taz_id = taz_data[\"taz_id\"].values[0]\n",
    "taz_data = taz_data.drop(columns=[\"one_time\", \"taz_id\", \"wkt_taz\"])\n",
    "taz_data.set_index(\"datetime\", inplace=True)\n",
    "for data_feature in features:\n",
    "    time_serie = taz_data[[data_feature]].asfreq(\"h\")\n",
    "    if np.all(time_serie == 0):\n",
    "        continue\n",
    "    res = MSTL(np.squeeze(time_serie), periods=[24, 24 * 7]).fit()\n",
    "    resids[str(taz_id) + \"_\" + data_feature] = res.resid\n",
    "\n",
    "    var_resid = np.var(res.resid)\n",
    "    var_observed = np.var(res.observed)\n",
    "    trend_strength = max(0, 1 - (var_resid / np.var(res.trend + res.resid)))\n",
    "    noise_strength = var_resid / var_observed\n",
    "\n",
    "    seasonal_individial_strengths = {}\n",
    "    for period in res.seasonal:\n",
    "        seasonal_individial_strengths[\"F_\" + str(period)] = max(\n",
    "            0, 1 - (var_resid / np.var(res.seasonal[period] + res.resid))\n",
    "        )\n",
    "    seasonal_strength = max(\n",
    "        0, 1 - (var_resid / np.var(res.seasonal.sum(axis=1) + res.resid))\n",
    "    )\n",
    "\n",
    "    stats_df = {\n",
    "        \"id\": taz_id,\n",
    "        \"Name\": taz_id_name[taz_id],\n",
    "        \"Feature\": data_feature,\n",
    "        \"F_T\": round(trend_strength, 3),\n",
    "        \"F_S\": round(seasonal_strength, 3),\n",
    "        \"F_R\": round(noise_strength, 3),\n",
    "    }\n",
    "\n",
    "    # add individual seasonal strengths to stats_df, rounded with 3 decimals\n",
    "    for period in seasonal_individial_strengths:\n",
    "        stats_df[period] = round(seasonal_individial_strengths[period], 3)\n",
    "\n",
    "    stats_table = pd.concat(\n",
    "        [stats_table, pd.DataFrame(stats_df, index=[0])], ignore_index=True\n",
    "    )\n",
    "\n",
    "pd.DataFrame(resids).to_csv(results_path + \"/resids.csv\", index=True)\n",
    "stats_table = stats_table.sort_values(by=[\"F_R\"], ascending=False)\n",
    "stats_table.to_csv(results_path + \"/decomposition_summary.csv\", index=False)\n",
    "stats_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# motif discovery\n",
    "import stumpy\n",
    "from stumpy import config\n",
    "\n",
    "config.STUMPY_EXCL_ZONE_DENOM = 2  # r = np.ceil(m/2)\n",
    "top_k_mp = 1\n",
    "include = None\n",
    "normalize = True\n",
    "subsequence_lengths = [12]\n",
    "\n",
    "resids = pd.read_csv(results_path + \"/resids.csv\", index_col=0)\n",
    "\n",
    "# plot the data and residuals of top 3 taz_ids\n",
    "top_taz_ids = stats_table[\"id\"].head(3).values\n",
    "# create dataframe with taz_id and resids\n",
    "top_resids = pd.DataFrame()\n",
    "for taz_id in top_taz_ids:\n",
    "    for data_feature in features:\n",
    "        # add to dataframe, id, data feature and daoly\n",
    "        observed = (\n",
    "            hourly_taz_data[hourly_taz_data[\"taz_id\"] == taz_id][\n",
    "                [data_feature, \"one_time\"]\n",
    "            ]\n",
    "            .sort_values(by=\"one_time\")\n",
    "            .set_index(\"one_time\")\n",
    "        )\n",
    "        index = observed.index.values\n",
    "        row = {\n",
    "            \"id\": taz_id,\n",
    "            \"feature\": data_feature,\n",
    "            \"index\": index,\n",
    "            \"observed\": observed.T.values[0],\n",
    "            \"residuals\": resids[str(taz_id) + \"_\" + data_feature],\n",
    "        }\n",
    "        top_resids = pd.concat([top_resids, pd.DataFrame(row)], ignore_index=True)\n",
    "\n",
    "## for (id, feature) get dataframe and plot observed and residuals\n",
    "# for i, df in top_resids.groupby(by=[\"id\", \"feature\"]):\n",
    "#    # plot observed and residuals\n",
    "#    fig, axes = plt.subplots(figsize=(5, 2), ncols=1, nrows=2, sharex=True)\n",
    "#    observed = pd.DataFrame(df[\"observed\"].values, index=df[\"index\"])\n",
    "#    resid = pd.DataFrame(df[\"residuals\"].values, index=df[\"index\"])\n",
    "#    observed.plot(ax=axes[0], legend=False, label=\"\", xlabel=\"\", ylabel=\"Observed\")\n",
    "#    resid.plot(ax=axes[1], style=\".\", legend=False, xlabel=\"\", ylabel=\"Residual\")\n",
    "#    axes[0].axes.get_xaxis().set_visible(False)\n",
    "#    plt.show()\n",
    "\n",
    "for m in subsequence_lengths:\n",
    "    X = []\n",
    "    for i, df in top_resids.groupby(by=[\"id\", \"feature\"]):\n",
    "        X.append(df[\"residuals\"].values)\n",
    "    X = np.array(X)\n",
    "    mp, indices = stumpy.mstump(X, m, include=include, normalize=normalize)\n",
    "    np.save(\n",
    "        results_path\n",
    "        + \"/mp/hourly_taz_normalize={}_topkmp={}_m={}_{}_multivar.npy\".format(\n",
    "            normalize, top_k_mp, m, i[0]\n",
    "        ),\n",
    "        mp,\n",
    "        allow_pickle=True,\n",
    "    )\n",
    "    np.save(\n",
    "        results_path\n",
    "        + \"/indices/hourly_taz_normalize={}_topkmp={}_m={}_{}_multivar.npy\".format(\n",
    "            normalize, top_k_mp, m, i[0]\n",
    "        ),\n",
    "        indices,\n",
    "        allow_pickle=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multivar_subsequence_complexity(x):\n",
    "    # complexity for multivariate time series can be calculated as the sum of the complexity of each dimension\n",
    "    return np.sum(np.sqrt(np.sum(np.square(np.diff(x)), axis=1)))\n",
    "\n",
    "\n",
    "def table_summary_motifs(\n",
    "    motif_indices,\n",
    "    motif_distances,\n",
    "    motif_subspaces,\n",
    "    data,\n",
    "    k_distances,\n",
    "    m,\n",
    "    normalize,\n",
    "    max_allowed_dist,\n",
    "):\n",
    "    mp_stats_table = pd.DataFrame(\n",
    "        columns=[\n",
    "            \"ID\",\n",
    "            \"k_distances\",\n",
    "            \"Features\",\n",
    "            \"m\",\n",
    "            \"#Matches\",\n",
    "            \"Indices\",\n",
    "            \"max(dists)\",\n",
    "            \"min(dists)\",\n",
    "            \"med(dists)\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    motif_index = 0\n",
    "\n",
    "    n_vars, n_time = data.shape\n",
    "\n",
    "    if normalize:\n",
    "        data = (data - np.mean(data, axis=1)[:, np.newaxis]) / np.std(data, axis=1)[\n",
    "            :, np.newaxis\n",
    "        ]\n",
    "\n",
    "    dtypes = [float] * len(data)\n",
    "    model_empirical = NullModel(data, dtypes, model=\"empirical\")\n",
    "\n",
    "    for motif_indice, match_indices in enumerate(motif_indices):\n",
    "        dimensions = motif_subspaces[motif_indice]\n",
    "\n",
    "        # remove filling values of -1 and Nans from motif_indices and match_distances\n",
    "        match_indices = match_indices[match_indices != -1]\n",
    "        match_distances = motif_distances[motif_indice]\n",
    "        match_distances = match_distances[~np.isnan(match_distances)]\n",
    "\n",
    "        # if is empty, skip\n",
    "        if len(match_indices) == 0:\n",
    "            continue\n",
    "\n",
    "        excl_zone = np.ceil(m / config.STUMPY_EXCL_ZONE_DENOM)\n",
    "\n",
    "        # remove trivial matches\n",
    "        non_trivial_matches = []\n",
    "        for indice in match_indices:\n",
    "            trivial = False\n",
    "            for indice_new in non_trivial_matches:\n",
    "                if abs(indice - indice_new) <= excl_zone:\n",
    "                    trivial = True\n",
    "                    break\n",
    "            if not trivial:\n",
    "                non_trivial_matches.append(indice)\n",
    "        match_indices = non_trivial_matches\n",
    "\n",
    "        max_possible_matches = int(np.floor((n_time - m) / excl_zone + 1))\n",
    "\n",
    "        # get the multidim time serie motif in the dimensions\n",
    "        multivar_subsequence = data[dimensions][\n",
    "            :, match_indices[0] : match_indices[0] + m\n",
    "        ]\n",
    "\n",
    "        # minmax normalize subsequence\n",
    "        epsilon = 1e-10  # to avoid division by zero\n",
    "        min_values = multivar_subsequence.min(axis=1, keepdims=True)\n",
    "        max_values = multivar_subsequence.max(axis=1, keepdims=True)\n",
    "        normalized_multivar_subsequence = (multivar_subsequence - min_values) / (\n",
    "            max_values - min_values + epsilon\n",
    "        )\n",
    "        ce_norm_subsequence = multivar_subsequence_complexity(\n",
    "            normalized_multivar_subsequence\n",
    "        )\n",
    "        norm_ce_norm_subsequence = ce_norm_subsequence / (\n",
    "            np.sqrt(len(multivar_subsequence[0]) - 1) * len(dimensions)\n",
    "        )\n",
    "\n",
    "        max_dist = np.max(match_distances)\n",
    "        min_dist = np.min(match_distances[1:])\n",
    "\n",
    "        if k_distances is None:  # consider all matches\n",
    "            med_dist = np.median(match_distances[1:])\n",
    "        else:  # consider only the k closest matches\n",
    "            med_dist = np.median(match_distances[1 : k_distances + 1])\n",
    "\n",
    "        # np.nanmax([np.nanmean(D) - 2.0 * np.nanstd(D), np.nanmin(D)])\n",
    "        if max_allowed_dist is None:\n",
    "            # D The distance profile of `Q` with `T`. It is a 1D numpy array of size\n",
    "            # `len(T)-len(Q)+1`, where `D[i]` is the distance between query `Q` and\n",
    "            # `T[i : i + len(Q)]`\n",
    "            D = np.empty((n_vars, n_time - m + 1))\n",
    "            for i in range(n_vars):\n",
    "                D[i, :] = stumpy.mass(\n",
    "                    multivar_subsequence[i], data[i], normalize=normalize\n",
    "                )\n",
    "            D = np.mean(D, axis=0)\n",
    "            D_copy = D.copy().astype(np.float64)\n",
    "            D_copy[np.isinf(D_copy)] = np.nan\n",
    "            motif_max_allowed_dist = np.nanmax(\n",
    "                [np.nanmean(D_copy) - 2.0 * np.nanstd(D_copy), np.nanmin(D_copy)]\n",
    "            )\n",
    "        else:\n",
    "            motif_max_allowed_dist = max_allowed_dist\n",
    "\n",
    "        unified_weights = \"0.33,0.33,0.33\"\n",
    "        w1, w2, w3 = map(float, unified_weights.split(\",\"))\n",
    "        unified = (\n",
    "            w1 * (1 - (med_dist / motif_max_allowed_dist))\n",
    "            + w2 * (len(match_indices) / max_possible_matches)\n",
    "            + w3 * norm_ce_norm_subsequence\n",
    "        )\n",
    "\n",
    "        # remove timepoints from time series in match all indices + m\n",
    "        time_series_nomatches = data.copy()\n",
    "        # list of indexes to remove\n",
    "        indexes_to_remove = [\n",
    "            i for index in match_indices for i in range(index, index + m)\n",
    "        ]\n",
    "        # put zero in the indexes to remove\n",
    "        time_series_nomatches[:, indexes_to_remove] = 0\n",
    "\n",
    "        # calculate variance explained by the motif\n",
    "        vars_explained = []\n",
    "        for i in range(len(dimensions)):\n",
    "            vars_explained.append(\n",
    "                100\n",
    "                * (\n",
    "                    1\n",
    "                    - (\n",
    "                        np.mean(np.abs(time_series_nomatches[i]))\n",
    "                        / np.mean(np.abs(data[i]))\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "        variance_explained = np.mean(vars_explained)\n",
    "\n",
    "        # data features are now the ones in the dimensions\n",
    "        used_features = [f\"{dimension}\" for dimension in dimensions]\n",
    "\n",
    "        # max_delta = motif_max_allowed_dist # (worst case) max_dist = sqrt(max_delta^2) <=> max_delta = max_dist\n",
    "        max_delta = math.sqrt(motif_max_allowed_dist**2 / m)\n",
    "        delta_thresholds = [max_delta] * len(data)\n",
    "\n",
    "        #########SIG#########\n",
    "        motif = Motif(\n",
    "            multivar_subsequence, dimensions, delta_thresholds, len(match_indices)\n",
    "        )\n",
    "        p = motif.set_pattern_probability(model_empirical, vars_indep=True)\n",
    "        pvalue = motif.set_significance(\n",
    "            max_possible_matches, n_vars, idd_correction=False\n",
    "        )\n",
    "\n",
    "        stats_df = {\n",
    "            \"ID\": str(motif_index),\n",
    "            \"k\": len(dimensions),\n",
    "            \"Features\": \",\".join(used_features),\n",
    "            \"m\": m,\n",
    "            \"#Matches\": len(match_indices) - 1,\n",
    "            \"Indices\": match_indices,\n",
    "            \"max(dists)\": np.around(max_dist, 3),\n",
    "            \"min(dists)\": np.around(min_dist, 3),\n",
    "            \"med(dists)\": np.around(med_dist, 3),\n",
    "            \"CE\": np.around(norm_ce_norm_subsequence, 3),\n",
    "            \"Score Unified\": np.around(unified, 3),\n",
    "            \"Explained Var(%)\": np.around(variance_explained, 2),\n",
    "            \"P\": p,\n",
    "            \"p-value\": pvalue,\n",
    "        }\n",
    "\n",
    "        mp_stats_table = (\n",
    "            pd.DataFrame.from_records([stats_df])\n",
    "            if mp_stats_table.empty\n",
    "            else pd.concat(\n",
    "                [mp_stats_table, pd.DataFrame.from_records([stats_df])],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        motif_index += 1\n",
    "    return mp_stats_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 São Domingos de Benfica (Estrada Luz | Oeste)\n",
      "37 São Domingos de Benfica (Estrada Luz | Oeste)\n",
      "37 São Domingos de Benfica (Estrada Luz | Oeste)\n",
      "37 São Domingos de Benfica (Estrada Luz | Oeste)\n",
      "37 São Domingos de Benfica (Estrada Luz | Oeste)\n",
      "37 São Domingos de Benfica (Estrada Luz | Oeste)\n",
      "m:12, #Motifs:21\n",
      "Sig  21\n",
      "Sig after Hochberg: 20, critical value: 1.8545684995084946e-106\n"
     ]
    }
   ],
   "source": [
    "k_distances = None\n",
    "min_neighbors = 2\n",
    "cutoff = [np.inf] * len(features)\n",
    "max_matches = 99999\n",
    "max_distance = None\n",
    "max_motifs = 99999\n",
    "k = None\n",
    "\n",
    "for m in subsequence_lengths:\n",
    "    X = []\n",
    "    for i, df in top_resids.groupby(by=[\"id\", \"feature\"]):\n",
    "        print(i[0], taz_id_name[i[0]])\n",
    "        mp_stats_table = pd.DataFrame()\n",
    "        X.append(df[\"residuals\"].values)\n",
    "    mp = np.load(\n",
    "        results_path\n",
    "        + \"/mp/hourly_taz_normalize={}_topkmp={}_m={}_{}_multivar.npy\".format(\n",
    "            normalize, top_k_mp, m, i[0]\n",
    "        ),\n",
    "        allow_pickle=True,\n",
    "    )\n",
    "    indices = np.load(\n",
    "        results_path\n",
    "        + \"/indices/hourly_taz_normalize={}_topkmp={}_m={}_{}_multivar.npy\".format(\n",
    "            normalize, top_k_mp, m, i[0]\n",
    "        ),\n",
    "        allow_pickle=True,\n",
    "    )\n",
    "    X = np.array(X)\n",
    "    motif_distances, motif_indices, motif_subspaces, motif_mdls = stumpy.mmotifs(\n",
    "        X,\n",
    "        mp,\n",
    "        indices,\n",
    "        min_neighbors=min_neighbors,\n",
    "        max_distance=max_distance,\n",
    "        cutoffs=cutoff,\n",
    "        max_matches=max_matches,\n",
    "        max_motifs=max_motifs,\n",
    "        k=k,\n",
    "        include=include,\n",
    "        normalize=normalize,\n",
    "    )\n",
    "    if len(motif_indices[0]) == 0:\n",
    "        continue\n",
    "    print(\"m:{}, #Motifs:{}\".format(m, len(motif_indices)))\n",
    "    table = table_summary_motifs(\n",
    "        motif_indices,\n",
    "        motif_distances,\n",
    "        motif_subspaces,\n",
    "        X,\n",
    "        k_distances,\n",
    "        m,\n",
    "        normalize,\n",
    "        max_distance,\n",
    "    )\n",
    "    print(\"Sig \", np.sum(table[\"p-value\"] < 0.001))\n",
    "    # hochberg procedure\n",
    "    p_values = table[\"p-value\"].to_numpy()\n",
    "    critical_value = NullModel.hochberg_critical_value(p_values, 0.05)\n",
    "    sig = (\n",
    "        table[\"p-value\"] < critical_value\n",
    "        if critical_value != 0\n",
    "        else table[\"p-value\"] <= critical_value\n",
    "    )\n",
    "    table[\"Sig_Hochber\"] = sig\n",
    "\n",
    "    print(\n",
    "        \"Sig after Hochberg: {}, critical value: {}\".format(np.sum(sig), critical_value)\n",
    "    )\n",
    "    mp_stats_table = (\n",
    "        table\n",
    "        if mp_stats_table.empty\n",
    "        else pd.concat([mp_stats_table, table], ignore_index=True)\n",
    "    )\n",
    "mp_stats_table.to_csv(\n",
    "    results_path\n",
    "    + \"/table_motifs_normalize={}_min_neighbors={}_max_distance={}_cutoff={}_max_matches={}_max_motifs={}_{}.csv\".format(\n",
    "        normalize,\n",
    "        min_neighbors,\n",
    "        max_distance,\n",
    "        cutoff,\n",
    "        max_matches,\n",
    "        max_motifs,\n",
    "        i[0],\n",
    "    ),\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 São Domingos de Benfica (Estrada Luz | Oeste)\n",
      "\\begin{tabular}{rrrrll}\n",
      "\\toprule\n",
      "m & #motifs & #sig_motifs(<0.001) & significant & avg_n_matches & avg_n_features \\\\\n",
      "\\midrule\n",
      "12 & 21 & 21 & 100.000 & (98.1, 21.865) & (6.0, 0.0) \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a new table for each motif length with statistics of the motifs (number of motifs found,\n",
    "# number of significant motifs, average number of matches +- std, average of features +- std,\n",
    "# average probability +- std, average pvalue +- std)\n",
    "\n",
    "for i, df in top_resids.groupby(by=[\"id\"]):\n",
    "    print(i[0], taz_id_name[i[0]])\n",
    "    mp_stats_table = pd.read_csv(\n",
    "        results_path\n",
    "        + \"/table_motifs_normalize={}_min_neighbors={}_max_distance={}_cutoff={}_max_matches={}_max_motifs={}_{}.csv\".format(\n",
    "            normalize,\n",
    "            min_neighbors,\n",
    "            max_distance,\n",
    "            cutoff,\n",
    "            max_matches,\n",
    "            max_motifs,\n",
    "            i[0],\n",
    "        )\n",
    "    )\n",
    "    motif_lengths = mp_stats_table[\"m\"].unique()\n",
    "    motif_stats_table = pd.DataFrame(\n",
    "        columns=[\n",
    "            \"m\",\n",
    "            \"#motifs\",\n",
    "            \"avg_n_matches\",\n",
    "            \"avg_n_features\",\n",
    "            \"avg_probability\",\n",
    "            \"avg_pvalue\",\n",
    "            \"#sig_motifs(<0.01)\",\n",
    "            \"significant\",\n",
    "            \"#sig_hochberg\",\n",
    "        ]\n",
    "    )\n",
    "    for m in motif_lengths:\n",
    "        table = mp_stats_table[mp_stats_table[\"m\"] == m]\n",
    "        if table.empty:\n",
    "            continue\n",
    "        n_motifs = table.shape[0]\n",
    "        n_sig_motifs_0001 = table[table[\"p-value\"] < 0.001].shape[0]\n",
    "        n_sig_motifs_hochberg = table[table[\"Sig_Hochber\"]].shape[0]\n",
    "        avg_n_matches = (\n",
    "            round(table[\"#Matches\"].mean(), 2),\n",
    "            round(table[\"#Matches\"].std(), 3),\n",
    "        )\n",
    "        avg_n_features = round(table[\"k\"].mean(), 2), round(table[\"k\"].std(), 3)\n",
    "        avg_probability = table[\"P\"].mean(), table[\"P\"].std()\n",
    "        avg_pvalue = table[\"p-value\"].mean(), table[\"p-value\"].std()\n",
    "\n",
    "        stats_df = {\n",
    "            \"m\": m,\n",
    "            \"#motifs\": n_motifs,\n",
    "            \"#sig_motifs(<0.001)\": n_sig_motifs_0001,\n",
    "            \"significant\": (n_sig_motifs_0001 * 100) / n_motifs,\n",
    "            \"avg_n_matches\": avg_n_matches,\n",
    "            \"avg_n_features\": avg_n_features,\n",
    "        }\n",
    "\n",
    "        motif_stats_table = (\n",
    "            pd.DataFrame.from_records([stats_df])\n",
    "            if motif_stats_table.empty\n",
    "            else pd.concat(\n",
    "                [motif_stats_table, pd.DataFrame.from_records([stats_df])],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    print(motif_stats_table.to_latex(index=False, float_format=\"%.3f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## taz_name:São Domingos de Benfica (Estrada Luz | Oeste) #########\n",
      "########## m:12 #########\n",
      "\\begin{tabular}{rrrlrrrrrlr}\n",
      "\\toprule\n",
      "ID & #Matches & k & Features & CE & Score Unified & max(dists) & min(dists) & med(dists) & p-value & Explained Var(%) \\\\\n",
      "\\midrule\n",
      "6 & 113 & 6 & 0,1,2,3,4,5 & 0.403 & 0.183 & 4.125 & 0.000 & 4.010 & 0.00e+00 & 27.200 \\\\\n",
      "12 & 91 & 6 & 0,1,2,3,4,5 & 0.354 & 0.180 & 3.580 & 0.000 & 3.248 & 0.00e+00 & 32.660 \\\\\n",
      "17 & 113 & 6 & 0,1,2,3,4,5 & 0.383 & 0.180 & 4.071 & 0.000 & 3.916 & 1.85e-106 & 14.270 \\\\\n",
      "8 & 95 & 6 & 0,1,2,3,4,5 & 0.388 & 0.177 & 3.963 & 0.000 & 3.794 & 0.00e+00 & 32.000 \\\\\n",
      "3 & 110 & 6 & 0,1,2,3,4,5 & 0.266 & 0.176 & 3.068 & 0.000 & 2.625 & 0.00e+00 & 27.750 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get top 3 most significant for each motif length\n",
    "for i, df in top_resids.groupby(by=[\"id\"]):\n",
    "    print(\"########## taz_name:{} #########\".format(taz_id_name[i[0]]))\n",
    "    mp_stats_table = pd.read_csv(\n",
    "        results_path\n",
    "        + \"/table_motifs_normalize={}_min_neighbors={}_max_distance={}_cutoff={}_max_matches={}_max_motifs={}_{}.csv\".format(\n",
    "            normalize,\n",
    "            min_neighbors,\n",
    "            max_distance,\n",
    "            cutoff,\n",
    "            max_matches,\n",
    "            max_motifs,\n",
    "            i[0],\n",
    "        )\n",
    "    )\n",
    "    # excluded p-value > 0.001\n",
    "    mp_stats_table = mp_stats_table[mp_stats_table[\"p-value\"] < 0.001]\n",
    "    subsequence_lengths = mp_stats_table[\"m\"].unique()\n",
    "    for m in subsequence_lengths:\n",
    "        print(\"########## m:{} #########\".format(m))\n",
    "        top_motifs = mp_stats_table[mp_stats_table[\"m\"] == m]\n",
    "        top_motifs = top_motifs.sort_values(by=\"Score Unified\", ascending=False).head(5)\n",
    "        top_motifs = top_motifs[\n",
    "            [\n",
    "                \"ID\",\n",
    "                \"#Matches\",\n",
    "                \"k\",\n",
    "                \"Features\",\n",
    "                \"CE\",\n",
    "                \"Score Unified\",\n",
    "                \"max(dists)\",\n",
    "                \"min(dists)\",\n",
    "                \"med(dists)\",\n",
    "                \"p-value\",\n",
    "                \"Explained Var(%)\",\n",
    "            ]\n",
    "        ]\n",
    "        top_motifs[\"p-value\"] = top_motifs[\"p-value\"].apply(lambda x: f\"{x:.2e}\")\n",
    "        print(top_motifs.to_latex(index=False, float_format=\"%.3f\"))\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_motif(ts_list, features, m, motif_indexes, motif_name, save_path):\n",
    "    fig, axes = plt.subplots(\n",
    "        ncols=3, nrows=len(ts_list), figsize=(10, 3 * len(ts_list)), squeeze=False\n",
    "    )\n",
    "    for i in range(0, len(ts_list)):\n",
    "        ts = ts_list[i]\n",
    "        # plot light grey\n",
    "        axes[i, 2].plot(ts, color=\"black\", linewidth=0.5, alpha=0.5)\n",
    "\n",
    "        colors = plt.cm.tab20(np.linspace(0, 1, len(motif_indexes)))\n",
    "        axes[i, 0].set_prop_cycle(\"color\", colors)\n",
    "        axes[i, 1].set_prop_cycle(\"color\", colors)\n",
    "        axes[i, 2].set_prop_cycle(\"color\", colors)\n",
    "\n",
    "        for index in motif_indexes:\n",
    "            subsequence_match = ts.iloc[index : index + m]\n",
    "            normalized_subsequence_match = (\n",
    "                subsequence_match - np.mean(subsequence_match)\n",
    "            ) / np.std(subsequence_match)\n",
    "            # original motif in the next plot with the same color\n",
    "            axes[i, 1].plot(subsequence_match.values)\n",
    "            # z-normalized motif in the next plot\n",
    "            axes[i, 0].plot(normalized_subsequence_match.values)\n",
    "            # highlight the motif in the original time serie\n",
    "            axes[i, 2].plot(subsequence_match, linewidth=2)\n",
    "\n",
    "        if m <= 12:\n",
    "            axes[i, 0].set_xticks(range(0, len(subsequence_match)))\n",
    "            axes[i, 0].set_xticklabels(\n",
    "                [\n",
    "                    \"i+\" + str(i) if i != 0 else \"i\"\n",
    "                    for i in range(0, len(subsequence_match))\n",
    "                ]\n",
    "            )\n",
    "            axes[i, 1].set_xticks(range(0, len(subsequence_match)))\n",
    "            axes[i, 1].set_xticklabels(\n",
    "                [\n",
    "                    \"i+\" + str(i) if i != 0 else \"i\"\n",
    "                    for i in range(0, len(subsequence_match))\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            axes[i, 0].set_xticks([0, 6, 12, 18, 23])\n",
    "            axes[i, 0].set_xticklabels([\"i\", \"i+6\", \"i+12\", \"i+18\", \"i+23\"])\n",
    "            axes[i, 1].set_xticks([0, 6, 12, 18, 23])\n",
    "            axes[i, 1].set_xticklabels([\"i\", \"i+6\", \"i+12\", \"i+18\", \"i+23\"])\n",
    "\n",
    "        # format the x axis to show the time and rotate for better reading\n",
    "        axes[i, 2].xaxis.set_major_formatter(mdates.DateFormatter(\"%m-%d\"))\n",
    "        # put y label if multivar\n",
    "        if len(ts_list) > 1:\n",
    "            axes[i, 0].set_ylabel(features[i], rotation=90, size=\"large\")\n",
    "\n",
    "        plt.setp(axes[i, 0].xaxis.get_majorticklabels(), rotation=90)\n",
    "        plt.setp(axes[i, 1].xaxis.get_majorticklabels(), rotation=90)\n",
    "        plt.setp(axes[i, 2].xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "    # title of the fig\n",
    "    axes[0, 0].set_title(\"Z-Normalized Subsequences\")\n",
    "    axes[0, 1].set_title(\"Raw Subsequences\")\n",
    "    axes[0, 2].set_title(\"Motif in Residual TS\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        save_path + \"/m=\" + str(m) + \"_motif_\" + str(motif_name) + \".pdf\",\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## taz_name:São Domingos de Benfica (Estrada Luz | Oeste) #########\n",
      "Motif length:  12\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '0,1,2,3,4,5'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m m \u001b[38;5;241m=\u001b[39m top_motif[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     29\u001b[0m dimensions \u001b[38;5;241m=\u001b[39m [top_motif[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m---> 30\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m([\u001b[38;5;28mint\u001b[39m(dimension) \u001b[38;5;28;01mfor\u001b[39;00m dimension \u001b[38;5;129;01min\u001b[39;00m dimensions])\n\u001b[1;32m     31\u001b[0m indices \u001b[38;5;241m=\u001b[39m top_motif[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndices\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m indices \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[0;32mIn[92], line 30\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     28\u001b[0m m \u001b[38;5;241m=\u001b[39m top_motif[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     29\u001b[0m dimensions \u001b[38;5;241m=\u001b[39m [top_motif[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m---> 30\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m([\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdimension\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m dimension \u001b[38;5;129;01min\u001b[39;00m dimensions])\n\u001b[1;32m     31\u001b[0m indices \u001b[38;5;241m=\u001b[39m top_motif[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndices\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m indices \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: '0,1,2,3,4,5'"
     ]
    }
   ],
   "source": [
    "# plot top motif\n",
    "for i, df in top_resids.groupby(by=[\"id\"]):\n",
    "    print(\"########## taz_name:{} #########\".format(taz_id_name[i[0]]))\n",
    "    mp_stats_table = pd.read_csv(\n",
    "        results_path\n",
    "        + \"/table_motifs_normalize={}_min_neighbors={}_max_distance={}_cutoff={}_max_matches={}_max_motifs={}_{}.csv\".format(\n",
    "            normalize,\n",
    "            min_neighbors,\n",
    "            max_distance,\n",
    "            cutoff,\n",
    "            max_matches,\n",
    "            max_motifs,\n",
    "            i[0],\n",
    "        )\n",
    "    )\n",
    "    mp_stats_table = mp_stats_table[mp_stats_table[\"p-value\"] < 0.001]\n",
    "    save_path = results_path + \"/\" + taz_id_name[i[0]]\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    subsequence_lengths = mp_stats_table[\"m\"].unique()\n",
    "    ts = pd.DataFrame(df[\"residuals\"].values, index=df[\"index\"])\n",
    "    ts.index = pd.to_datetime(ts.index)\n",
    "    for m in subsequence_lengths:\n",
    "        print(\"Motif length: \", m)\n",
    "        top_motifs = mp_stats_table[mp_stats_table[\"m\"] == m]\n",
    "        top_motifs = top_motifs.sort_values(by=\"Score Unified\", ascending=False).head(5)\n",
    "        for top_motif in top_motifs.to_dict(orient=\"records\"):\n",
    "            m = top_motif[\"m\"]\n",
    "            dimensions = [top_motif[\"Features\"]]\n",
    "            features = sorted([int(dimension) for dimension in dimensions])\n",
    "            indices = top_motif[\"Indices\"].replace(\"[\", \"\").replace(\"]\", \"\").split(\",\")\n",
    "            indices = [int(i) for i in indices]\n",
    "            motif_name = str(i[0]) + \"_\" + str(i[1]) + \"_\" + str(top_motif[\"ID\"])\n",
    "            plot_motif([ts], features, m, indices, motif_name, save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "motifsinresidualenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
